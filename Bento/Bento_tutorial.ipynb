{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate bento\n",
    "# !pip install bentoml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bentoml import BentoService, api, artifacts\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.tensorflow import TensorflowSavedModelArtifact\n",
    "\n",
    "@artifacts([TensorflowSavedModelArtifact('tts_model')])\n",
    "class TTSService(BentoService):\n",
    "\n",
    "    @api(input=JsonInput(), batch=False)\n",
    "    def predict(self, json_input):\n",
    "        # Load the input data and perform inference with the loaded model\n",
    "        input_data = json_input['input']\n",
    "        model = self.artifacts.tts_model\n",
    "        output = model.inference(input_data)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlowTTS를 BentoML과 함께 사용할 수 있습니다. \n",
    "- TensorFlowTTS가 설치된 Docker 컨테이너를 이미 생성했으므로 이전처럼 Dockerfile을 사용하여 컨테이너를 BentoML 서비스로 패키징한 다음 Kubernetes 또는 AWS Elastic Beanstalk와 같은 컨테이너 기반 배포 플랫폼에 배포할 수 있습니다.\n",
    "- Docker 컨테이너를 BentoML 서비스로 패키징하려면 BentoService 클래스와 TensorflowSavedModelArtifact를 사용하여 저장된 TensorFlowTTS 모델을 로드할 수 있습니다. 다음은 예제 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from bentoml import BentoService, api, artifacts\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.tensorflow import TensorflowSavedModelArtifact\n",
    "from tensorflow_tts.inference import TFAutoModel\n",
    "\n",
    "@artifacts([TensorflowSavedModelArtifact('tts_model')])\n",
    "class TTSService(BentoService):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tts_model = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.tts_model = TFAutoModel.from_pretrained(self.artifacts.tts_model)\n",
    "\n",
    "    @api(input=JsonInput(), batch=False)\n",
    "    def predict(self, json_input):\n",
    "        # Load the input data and perform inference with the loaded model\n",
    "        input_data = json_input['input']\n",
    "        output = self.tts_model(input_data, training=False)[0]\n",
    "\n",
    "        return output.numpy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 예제 코드에서는 TensorFlowTTS.inference의 TFAutoModel을 사용하여 저장된 모델을 로드합니다. \n",
    "- 그런 다음 JSON 입력을 받고 로드된 모델로 추론을 수행하는 예측 메서드를 정의합니다. 예측 방법의 출력은 numpy 배열 형식으로 생성된 음성 파형입니다.\n",
    "\n",
    "- 그런 다음 새 BentoML 서비스 인스턴스를 만들고 다음을 실행하여 디렉터리에 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_service = TTSService()\n",
    "tts_service.pack('tts_model', '/path/to/saved_model')\n",
    "saved_path = tts_service.save()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이렇게 하면 TensorFlowTTS 모델을 아티팩트로 포함하는 saved_path에 BentoML 서비스 번들이 생성됩니다. \n",
    "\n",
    "- 그런 다음 저장된 BentoML 서비스를 로드하고 다음과 같이 예측 메서드를 호출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bentoml import load\n",
    "\n",
    "loaded_service = load(saved_path)\n",
    "output = loaded_service.predict({'input': input_data})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bentoml에 필요한 패키지를 가져오고 TTS.py 파일을 수정하여 Bentoml 서비스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "from bentoml import BentoService, api, artifacts\n",
    "from bentoml.adapters import JsonInput\n",
    "from bentoml.frameworks.tensorflow import TensorflowSavedModelArtifact\n",
    "from tensorflow_tts.inference import AutoProcessor\n",
    "from tensorflow_tts.inference import TFAutoModel\n",
    "\n",
    "\n",
    "@artifacts([TensorflowSavedModelArtifact('tts_model')])\n",
    "class TTSService(BentoService):\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        processor = AutoProcessor.from_pretrained(\"tensorspeech/tts-tacotron2-kss-ko\")\n",
    "        input_ids = processor.text_to_sequence(text)\n",
    "        return input_ids\n",
    "\n",
    "    def predict(self, text):\n",
    "        # Load the input data and perform inference with the loaded model\n",
    "        input_ids = self.preprocess(text)\n",
    "        tacotron2 = self.artifacts.tts_model['tacotron2']\n",
    "        mb_melgan = self.artifacts.tts_model['mb_melgan']\n",
    "        decoder_output, mel_outputs, stop_token_prediction, alignment_history = tacotron2.inference(input_ids=tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),input_lengths=tf.convert_to_tensor([len(input_ids)], tf.int32),speaker_ids=tf.convert_to_tensor([0], dtype=tf.int32),)\n",
    "        audio = mb_melgan.inference(mel_outputs)[0, :, 0]\n",
    "        # save to file\n",
    "        sf.write('./audio.wav', audio, 22050, \"PCM_16\")\n",
    "        return './audio.wav'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Dockerfile에서 다음 줄을 추가하여 bentoml 패키지를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN pip install bentoml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Bentoml을 사용하도록 Entrypoint 을 수정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRYPOINT [\"bentoml\", \"serve-gunicorn\", \"/bento\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Docker image 를 Build 합니다. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker build -t tts-service ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. save 커맨드를 사용해서 당신의 Bentoml service 를 저장합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bentoml import save\n",
    "\n",
    "service = TTSService()\n",
    "service.pack('tts_model', {'tacotron2': tacotron2, 'mb_melgan': mb_melgan})\n",
    "save(service, 'bento')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Docker run 커맨드를 사용해서 container 을 build 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo docker run -p 5000:5000 -v /path/to/bento:/bento tts-service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. curl 또는 HTTP 클라이언트를 사용하여 API에 요청합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -i \\\n",
    "    --header \"Content-Type: application/json\" \\\n",
    "    --request POST \\\n",
    "    --data '{\"text\": \"550호 아저씨 95000원 내세요.\"}' \\\n",
    "    http://localhost:5000/predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl은 명령줄에서 사용할 수 있는 HTTP client이며, 다양한 프로토콜을 지원하며, 데이터 전송을 위해 다양한 기능을 제공합니다. HTTP client는 서버에 HTTP 요청을 보내고, 서버가 응답한 데이터를 받아오는 도구입니다.\n",
    "\n",
    "예를 들어, curl을 사용하여 Google의 메인 페이지를 가져올 수 있습니다. 다음과 같은 명령어를 사용합니다.\n",
    "\n",
    "curl https://www.google.com\n",
    "\n",
    "\n",
    "이 명령어는 HTTPS를 사용하여 https://www.google.com 주소로 GET 요청을 보내고, 서버로부터 받은 응답 데이터를 출력합니다.\n",
    "\n",
    "HTTP client를 사용하면 서버에 요청을 보내고, 서버가 응답한 데이터를 받아와서 처리할 수 있습니다. Python에서는 requests와 urllib 같은 라이브러리를 사용하여 HTTP client를 구현할 수 있습니다. 이 라이브러리들은 URL을 열고 데이터를 읽어오는 간단한 인터페이스를 제공합니다.\n",
    "\n",
    "예를 들어, requests 라이브러리를 사용하여 Google의 메인 페이지를 가져오는 코드는 다음과 같습니다.\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://www.google.com')\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "이 코드는 requests.get() 함수를 사용하여 HTTPS를 사용하여 https://www.google.com 주소로 GET 요청을 보내고, 서버로부터 받은 응답 데이터를 출력합니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
