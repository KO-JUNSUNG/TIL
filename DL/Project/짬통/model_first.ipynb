{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 16:01:23.030359: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-02 16:01:23.723966: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/extras/CUPTI/lib64:/usr/local/cuda-11.2/lib64:\n",
      "2023-02-02 16:01:23.726141: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/extras/CUPTI/lib64:/usr/local/cuda-11.2/lib64:\n",
      "2023-02-02 16:01:23.726152: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "## import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score # Accuracy metrics \n",
    "\n",
    "## This is for data handling.\n",
    "# import cv2\n",
    "# import time\n",
    "# import math as m\n",
    "# import MediaPipe as mp\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use MediaPipe to make a Convolutional Neural Network (CNN) model for pose prediction, you would follow these steps:\n",
    "\n",
    "1. Install MediaPipe: To use MediaPipe, you'll first need to install it. You can do this by following the installation instructions on the MediaPipe GitHub repository.\n",
    "\n",
    "2. Gather data: You'll need a dataset of human poses to train your CNN model. You can either gather your own data or use a publicly available dataset.\n",
    "\n",
    "3. Preprocess data: Before training the CNN model, you'll need to preprocess the data to ensure that it's in the correct format and has been normalized.\n",
    "\n",
    "4. Define the CNN model architecture: In this step, you'll need to define the structure of the CNN model, including the number of layers, the type of layers, and the activation functions.\n",
    "\n",
    "5. Train the CNN model: Using the preprocessed data, you'll train the CNN model using a suitable deep learning framework such as TensorFlow or PyTorch.\n",
    "\n",
    "6. Evaluate the CNN model: After training the model, you'll need to evaluate its performance on a validation dataset to ensure that it's working as expected.\n",
    "\n",
    "- Use MediaPipe to run the CNN model: Once the CNN model is trained and working, you can use MediaPipe to integrate it into a processing pipeline. To do this, you'll need to write code that initializes the pipeline, sets up the input and output streams, and starts the processing loop.\n",
    "\n",
    "- Note: The exact steps to use MediaPipe for pose prediction may vary depending on the specific pipeline you're using and the programming language you're using. I suggest that you refer to the official MediaPipe documentation and other relevant resources for more detailed instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just REFERENCE 1 코드\n",
    "# 데이터 전처리\n",
    "# 4개의 파일을 사용\n",
    "data_stand_woman = np.loadtxt(fileUrl + '/stand/data_stand_woman.csv', unpack = True, delimiter=',', skiprows = 1, dtype=np.int64)\n",
    "data_stand_man = np.loadtxt(fileUrl + '/stand/data_stand_man.csv', unpack = True, delimiter=',', skiprows = 1, dtype=np.int64)\n",
    "data_sit_woman = np.loadtxt(fileUrl + '/sit/data_sit_woman.csv', unpack = True, delimiter=',', skiprows = 1, dtype=np.int64)\n",
    "data_sit_man = np.loadtxt(fileUrl + '/sit/data_sit_man.csv', unpack = True, delimiter=',', skiprows = 1, dtype=np.int64)\n",
    "\n",
    "# 두 명의 서 있는 자세와 앉아 있는 자세 병합\n",
    "data_stand = np.concatenate((data_stand_woman, data_stand_man), axis = 1).transpose()\n",
    "data_sit = np.concatenate((data_sit_woman, data_sit_man), axis = 1).transpose()\n",
    "\n",
    "# 분류를 위한 라벨 구성 (0 - 서 있는 자세, 1 - 앉아 있는 자세)\n",
    "label_stand = []\n",
    "label_sit = []\n",
    "for i in range(len(data_stand)):\n",
    "  label_stand.append(0)\n",
    "for i in range(len(data_sit)):\n",
    "  label_sit.append(1)\n",
    "\n",
    "# 앉아 있는 자세와 서 있는 자세에 대한 데이터를 합쳐서 데이터 전처리 마무리\n",
    "data = np.concatenate((data_stand, data_sit), axis = 0)\n",
    "label = np.array(label_stand + label_sit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size = 0.2, random_state = 121)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first Model 2023/2/02\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, input_shape = (26,), activation='relu'), # 26개의 관절 좌표\n",
    "    keras.layers.Dense(10, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "model.fit(X_train, y_train, batch_size = 50, epochs = 50, verbose = 1) # train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 \n",
    "# I made it.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3))) #BGR or RGB\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3,3), activation='relu'))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax')) # normal, or alert\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on your training data\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 2\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate 2\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Maded_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3, Just Reference 2 code, hand predict mediapipe model\n",
    "# Reference 2 = https://github.com/Rishav-hub/MediaPipe-Tutorial/blob/main/MediaPipe.ipynb\n",
    "# It used several ML model for prediction.\n",
    "\n",
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "} #default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 3 \n",
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 3\n",
    "\n",
    "for algo, model in fit_models.items():\n",
    "    predicted_y = model.predict(X_test)\n",
    "    print(algo, accuracy_score(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # real check\n",
    "\n",
    "# with open('Numbers.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# with mp.solutions.pose(min_detection_confidence=0.5,min_tracking_confidence=0.5) as mp_pose:\n",
    "#     while cap.isOpened():\n",
    "#         success, image = cap.read()\n",
    "       \n",
    "#         # Flip the image horizontally for a later selfie-view display, and convert\n",
    "#         # the BGR image to RGB.\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         # To improve performance, optionally mark the image as not writeable to\n",
    "#         # pass by reference.\n",
    "#         image.flags.writeable = False\n",
    "#         results = mp_pose.process(image)\n",
    "\n",
    "#         # Draw the hand annotations on the image.\n",
    "#         image.flags.writeable = True\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "# #         image_height, image_width, _ = image.shape\n",
    "\n",
    "#         if results.multi_hand_landmarks:\n",
    "#             for hand_landmarks in results.multi_hand_landmarks:\n",
    "#                 mp_drawing.draw_landmarks(\n",
    "#                     image, hand_landmarks, mp_pose.HAND_CONNECTIONS)\n",
    "#         try:\n",
    "#             hand = hand_landmarks.landmark\n",
    "#             hand_row = list(np.array([[landmark.x, landmark.y, landmark.z] for landmark in hand]).flatten())\n",
    "\n",
    "#             row = hand_row\n",
    "# #             print(row)\n",
    "\n",
    "\n",
    "#             X = pd.DataFrame([row])\n",
    "#             hand_language_class = model.predict(X)[0]\n",
    "#             hand_language_prob = model.predict_proba(X)[0]\n",
    "#             print(hand_language_class, hand_language_prob)\n",
    "\n",
    "\n",
    "\n",
    "#             coords = tuple(np.multiply(\n",
    "#                             np.array(\n",
    "#                                 (hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].x, \n",
    "#                                  hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y))\n",
    "#                         , [640,480]).astype(int))\n",
    "\n",
    "#             cv2.rectangle(image, \n",
    "#                           (coords[0], coords[1]+5), \n",
    "#                           (coords[0]+len(hand_language_class)*20, coords[1]-30), \n",
    "#                           (245, 117, 16), -1)\n",
    "#             cv2.putText(image, hand_language_class, coords, \n",
    "#                         cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#             # Get status box\n",
    "#             cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "\n",
    "#             # Display Class\n",
    "#             cv2.putText(image, 'CLASS'\n",
    "#                         , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "#             cv2.putText(image, hand_language_class.split(' ')[0]\n",
    "#                         , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#             # Display Probability\n",
    "#             cv2.putText(image, 'PROB'\n",
    "#                         , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "#             cv2.putText(image, str(round(hand_language_prob[np.argmax(hand_language_prob)],2))\n",
    "#                         , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "#         except:\n",
    "#             pass\n",
    "            \n",
    "   \n",
    "#         cv2.imshow('MediaPipe Hands 2', image)\n",
    "#         if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#             break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71be762759ae1b1747e1cfc60efa181cfc93ac416fe5084a790ec64e97c902b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
