{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#torch.nn 문서 읽기\n",
        "``` python\n",
        "설명에 그래프를 만들기 위한 \"basic building block\"이라고 쓰여져 있습니다!\n",
        "\n",
        "저희가 다양한 함수들을 잘 활용하면\n",
        "여기서 말하는 \"basic building block\"을 만들 수 있겠지만 시간이 걸리니까\n",
        "PyTorch에서 미리 만들어두고 이를 \"torch.nn\"으로 묶어 쉽게 사용하게 만들었어요! \n",
        "\n",
        "여기서 제공해주는 블럭들을 잘 이용하면 그래프라는 딥러닝 모델을 만들 수 있을 것 같아요!\n",
        "\n",
        "매우 중요해보이지만 내용이 정말 많네요!\n",
        "이 많은 내용을 다 볼 수는 없으니까 간단하게 훑어보겠습니다!\n",
        "```\n",
        "\n",
        "- [torch.nn 문서 - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html)"
      ],
      "metadata": {
        "id": "x9ornn1kBsfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch.nn `Linear Layers`\n",
        "\n",
        "``` python\n",
        "가볍게 읽기만 하려고 했는데 그래도 한 두개는 직접 예제를 따라치면 좋을 것 같아요!\n",
        "딥러닝을 공부하면, y = WX + b 라는 공식을 자주 볼거예요!\n",
        "이 linear transformation을 구현해놓은 \"nn.Linear\"가 속해잇는\n",
        "\"Linear Layers\" 항목을 잠깐만 같이 살펴보겠습니다!\n",
        "```\n",
        "\n",
        "- [torch.nn Linear Layers - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html#linear-layers)"
      ],
      "metadata": {
        "id": "daSv9oMiEsca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Linear\n",
        "\n",
        "\n",
        "``` python\n",
        "pytorch로 딥러닝 설계를 한다면, 이 \"nn.Linear\"는 정말 많이 보게 될거예요!\n",
        "```\n",
        "\n",
        "- [torch.nn.Linear - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
        "\n",
        "**힌트**\n",
        "- PyTorch에는 tensor 크기(or 모양)를 반환하는 함수가 있어요! 영어로 크기가 무엇일까요?"
      ],
      "metadata": {
        "id": "jzLfuiETE-gW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P-Xg4tHUBRFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2f7310-7f2f-459d-9d66-db23380f00bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "X = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "# TODO : tensor X의 크기는 (2, 2)입니다\n",
        "#        nn.Linear를 사용하여서 (2, 5)로 크기를 바꾸고 이 크기를 출력하세요!\n",
        "\n",
        "linear = nn.Linear(2,5)\n",
        "output = linear(X) # 저장된 객체에 X를 호출 (함수에 X를 적용)\n",
        "output.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### nn.Identity\n",
        "이 layer도 유용하게 사용됩니다. 다만 딥러닝을 막 배우는 단계에서 이 layer를 사용할 일은 거의 없기 때문에 사용처를 아실 필요는 없습니다. 다만 궁금해하실 분들을 위해 링크를 남겨놓습니다.\n",
        "\n",
        "``` python\n",
        "\"nn.Identity\"는 입력과 출력이 동일하게 나오는데 도대체 왜 만들어놓은 걸까요?\n",
        "그래도.. 한번 사용해봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn.Identity - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Identity.html#torch.nn.Identity)\n",
        "\n",
        "**유용한 자료**\n",
        "- [What is the use of nn.Identity? - PyTorch Forum](https://discuss.pytorch.org/t/what-is-the-use-of-nn-identity/51781)\n",
        "- [What is the idea behind using nn.Identity for residual learning? - Stack Overflow](https://stackoverflow.com/questions/64229717/what-is-the-idea-behind-using-nn-identity-for-residual-learning)"
      ],
      "metadata": {
        "id": "6pfGOu3CGJJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "X = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "# TODO : nn.Identity를 생성해 X를 입력시킨 후 나온 출력값이 X와 동일한지 확인해보세요!\n",
        "identity = nn.Identity()\n",
        "output = identity(X)\n",
        "output"
      ],
      "metadata": {
        "id": "NGlUb2HbGXeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628d4d51-6a2d-45d1-f9a9-582cb59bcb0f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom 모델 제작을 위한 nn.Module 클래스\n",
        "\n",
        "```\n",
        "PyTorch 라이브러리가 제공해주는 다양한 기능들과 nn.Module를 활용하여 모델 제작 및 분석을 진행해볼 것입니다!\n",
        "```\n",
        "\n",
        "PyTorch가 제공해주는 기능들을 조합하여서 모델을 만들 차례입니다. 모델을 만들기 위해서 기능들을 단순히 나열해놓기만 한다면 지저분하겠죠? 그래서 PyTorch는 이런 일련의 기능들을 한 곳에 모아 하나의 모델로 추상화할 수 있게끔 클래스를 제공합니다.\n",
        "\n",
        "```\n",
        "nn.Module\n",
        "```\n",
        "- [torch.nn.Module - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
        "\n",
        "`nn.Module` 클래스는 여러 기능들을 한 곳에 모아놓는 상자 역할을 합니다.<br>\n",
        "`nn.Module`이라는 상자는 다른 `nn.Module` 상자를 포함할 수도 있습니다!<br>\n",
        "어떻게 사용햐느냐에 따라 `nn.Module` 상자는 다른 의미를 가집니다.\n",
        "\n",
        "- `nn.Module`이라는 상자에 `기능`들을 가득 모아놓은 경우 `basic building block`\n",
        "- `nn.Module`이라는 상자에 `basic building block`인 `nn.Module`들을 가득 모아놓은 경우 `딥러닝 모델`\n",
        "- `nn.Module`이라는 상자에 `딥러닝 모델`인 `nn.Module`들을 가득 모아놓은 경우 `더욱 큰 딥러닝 모델`\n",
        "\n",
        "`nn.Module`은 빈 상자일 뿐 이를 어떻게 사용할지는 온전히 설계자의 몫입니다!<br>\n",
        "`기능`과 `basic building block`과 `딥러닝 모델`을 혼재해서 마구잡이로 담을 수도 있고<br>\n",
        "`기능`은 `기능`끼리 `block`은 `block`끼리 계층적으로 담을 수도 있습니다!\n",
        "\n",
        "우리는 여기서 `nn.Module`를 이용해 모델을 제작해보고 제작한 모델이 어떻게 구성되어있는지 분석해볼 것입니다!<br>\n",
        "추가적으로 custom 모델 제작에 유용할 수 있는 `nn.Module`의 기능들도 살펴볼 것입니다!\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)"
      ],
      "metadata": {
        "id": "6fF_tccTGy6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Documentation에서 nn.Module을 검색해서 찾으세요!\n",
        "- nn.Module 문서의 설명을 읽어보세요!\n",
        "    - ![](https://github.com/IllgamhoDuck/PyTorch/blob/main/document_image/nn.Module.png?raw=true)\n",
        "- nn.Module 내부의 method들의 이름과 설명을 가볍게 훑어보세요!"
      ],
      "metadata": {
        "id": "0ZmopkDNHgHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nn.Module 모델 제작"
      ],
      "metadata": {
        "id": "mHlNQAiURWKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"nn.Module\"를 이용해서 더하기 연산을 하는 모델을 만들어봅시다!"
      ],
      "metadata": {
        "id": "CGtgXHBFH77c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : Add 모델을 완성하세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self):\n",
        "        # TODO : init 과정에서 반드시 들어가야 하는 super 관련 코드가 있습니다\n",
        "        # 자식 클래스(Student)가 상속받는 부모 클래스(Human)를 자식 클래스(Student)에 불러오겠다\n",
        "        # super().__init__() 의 역할 즉, 자식 클래스에도 부모 클래스의 인스턴스 속성과 동일한 속성이 생성\n",
        "        super().__init__()\n",
        "    def forward(self, x1, x2):\n",
        "        # TODO : torch.add 함수를 사용해서 더하기 연산을 해주세요!\n",
        "        output = torch.add(x1,x2)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x1 = torch.tensor([1])\n",
        "x2 = torch.tensor([2])\n",
        "\n",
        "add = Add()\n",
        "output = add(x1, x2)\n",
        "\n",
        "output #3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuAQ3RqDG8vg",
        "outputId": "e5e6f81a-46a6-4de8-e1e2-7aa38744bae6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "super를 통해서 init을 하는 것은 왜 그런걸까요?\n",
        "\n",
        "아래 링크의 글을 읽으니까 의문이 좀 풀리는 것 같아요! \n",
        "```\n",
        "\n",
        "**유용한 자료**\n",
        "- [Why is the super constructor necessary in PyTorch custom modules? - Stack Overflow](https://stackoverflow.com/questions/63058355/why-is-the-super-constructor-necessary-in-pytorch-custom-modules)"
      ],
      "metadata": {
        "id": "l1IXdUATIN4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Container\n",
        "\n",
        "``` python\n",
        "저희가 원하는 모듈(Module)을 만들었습니다!\n",
        "\n",
        "이렇게 만든 모듈(Module)들을 묶어서 사용하고 싶은데 어떻게 하는걸까요?\n",
        "파이썬의 리스트에 모듈들을 보관하면 되는걸까요?\n",
        "\n",
        "Documentation을 열심히 찾아보니까 관련된 함수들이\n",
        "\"torch.nn\"의 Container 항목에 포함되어있네요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [Containers  - PyTorch 공식 문서](https://pytorch.org/docs/stable/nn.html#containers)"
      ],
      "metadata": {
        "id": "181G00SiIg6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### torch.nn.Sequential\n",
        "\n",
        "``` python\n",
        "모듈(Module)들을 하나로 묶어 순차적으로 실행시키고 싶을때 torch.nn.Sequential를 사용한다고 합니다!\n",
        "```\n",
        "\n",
        "- [torch.nn.Sequential - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)"
      ],
      "metadata": {
        "id": "V7p23EsLIq1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "# TODO : 위에 모듈(Module)과 nn.Sequential를 이용해서\n",
        "#        입력값 x가 주어지면 다음의 연산을 처리하는 모델을 만들어보세요!\n",
        "#        y = x + 3 + 2 + 5\n",
        "calculator = nn.Sequential(Add(3),\n",
        "                           Add(2),\n",
        "                           Add(5),)\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "output = calculator(x)\n",
        "\n",
        "output # 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Vz0pLnIE6n",
        "outputId": "c69737a9-320f-4b69-c444-14e3fb32c96f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([11])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### torch.nn.ModuleList\n",
        "\n",
        "``` python\n",
        "torch.nn.Sequential은 묶어놓은 모듈들을 차례대로 수행하기 때문에 실행 순서가 정해져있는 기능들을 하나로 묶어두기 좋아보입니다!\n",
        "\n",
        "하지만 파이썬의 list처럼 모아두기만 하고 그때그때 원하는 것만\n",
        "인덱싱(indexing)을 통해 쓰고 싶으면 torch.nn.ModuleList을 쓰면 되지 않을까요?\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleList - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList)"
      ],
      "metadata": {
        "id": "fYzmmk7RKpL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : self.add_list에 담긴 모듈들을 이용하여서\n",
        "        #        y = ((x + 3) + 2) + 5 의 연산을 구현하세요!\n",
        "        x = self.add_list[1](x) # Add 객체이므로 value 를 저장하기 위한 입력 x를 받아야겠지?\n",
        "        x = self.add_list[0](x)\n",
        "        x = self.add_list[2](x)\n",
        "     \n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "calculator = Calculator()\n",
        "output = calculator(x)\n",
        "\n",
        "output # 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWMLPX2pIwY-",
        "outputId": "d3eb4983-40fd-4bf9-b972-9ce8bcf2c83e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([11])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  torch.nn.ModuleDict\n",
        "\n",
        "``` python\n",
        "torch.nn.ModuleLists 정말 편리하네요!!\n",
        "하지만 만약 리스트에 담긴 모듈의 크기가 정말 커진다면 나중에는 인덱싱(indexing)으로 원하는 모듈을 찾기가 정말 힘들어질 것 같아요!\n",
        "\n",
        "파이썬의 dict처럼 특정 모듈을 key값을 이용해 보관해놓는다면 나중에 원하는 모듈을 가져올때 훨씬 수월하지 않을까요?\n",
        "마침 PyTorch에 torch.nn.ModuleDict이 있네요! 같이 써봐요!\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleDict - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict)"
      ],
      "metadata": {
        "id": "a-n5eTPPKxZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.add_dict = nn.ModuleDict({'add2': Add(2),\n",
        "                                       'add3': Add(3),\n",
        "                                       'add5': Add(5)})\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : self.add_dict에 담긴 모듈들을 이용하여서\n",
        "        #        y = ((x + 3) + 2) + 5 의 연산을 구현하세요!\n",
        "\n",
        "        x = self.add_dict['add3'](x) # dict 의 인덱스 호출과 똑같다.\n",
        "        x = self.add_dict['add2'](x)\n",
        "        x = self.add_dict['add5'](x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "calculator = Calculator()\n",
        "output = calculator(x)\n",
        "\n",
        "output # 11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvLi8HHVKv_e",
        "outputId": "cc7cc67e-c0b9-47d1-febe-cf83d7247ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([11])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='red'><b>[ 퀴즈 ]</b></font> Python List vs PyTorch ModuleList\n",
        "``` python\n",
        "그런데 가만히 생각해보니까 파이썬에도 List가 있는데 왜 굳이 PyTorch에서는 ModuleList를 별도로 만들어두었을까요?\n",
        "\n",
        "그 이유가 궁금해요!\n",
        "```\n",
        "\n",
        "- [torch.nn.ModuleList - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList)\n",
        "\n",
        "**힌트**\n",
        "- 아래에 작성된 코드를 실행시키시면 힌트를 얻을 수 있습니다!"
      ],
      "metadata": {
        "id": "Gh2d5DjjK_2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "\n",
        "class PythonList(nn.Module):\n",
        "    \"\"\"Python List\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Python List\n",
        "        self.add_list = [Add(2), Add(3), Add(5)]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.add_list[1](x)\n",
        "        x = self.add_list[0](x)\n",
        "        x = self.add_list[2](x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class PyTorchList(nn.Module):\n",
        "    \"\"\"PyTorch List\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Pytorch ModuleList\n",
        "        self.add_list = nn.ModuleList([Add(2), Add(3), Add(5)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.add_list[1](x)\n",
        "        x = self.add_list[0](x)\n",
        "        x = self.add_list[2](x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "RWuptUP-K544"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([1])\n",
        "\n",
        "python_list = PythonList()\n",
        "pytorch_list = PyTorchList()\n",
        "\n",
        "# 기능 동작은 동일합니다!\n",
        "print(python_list(x), pytorch_list(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9e7CpKMLHON",
        "outputId": "2de864fb-81b7-4c37-b282-88b57ec7f9d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11]) tensor([11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python List로 모아놓은 모듈들이 감쪽같이 사라졌습니다!\n",
        "python_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijEW2bs9LIOq",
        "outputId": "08162ffd-883b-4c1d-fa08-b5ad0405ed77"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonList()"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하지만 PyTorch의 ModuleList로 모아놓은 모듈들은 짠! 하고 나타나네요!\n",
        "pytorch_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWOZMZG2LJO6",
        "outputId": "68aa5ef3-1831-47c7-847d-4943c7376f3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PyTorchList(\n",
              "  (add_list): ModuleList(\n",
              "    (0): Add()\n",
              "    (1): Add()\n",
              "    (2): Add()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "\n",
        "\n",
        "python_list는 클래스 자체(Python_list())를 반환하지만, pytorch_list는 클래스 내의 ModuleList 정보를 함께 반환한다.\n",
        "\n",
        "기능적으로는 동일하지만 nn.Module 의 submodule 로 등록이 되느냐 마느냐의 차이가 발생하는데, 이 차이로 인해 위의 차이가 발생된다.\n",
        "\n",
        "더 자세히 말하면, nn Module 내부에서 새로운 변수를 만들 때 '변수 = 값'의 형태로 코드를 적으면 '__setattr__' 의 특수 메서드가 호출된다.\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "tkR4bqOdNd1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 조건문\n",
        "\n",
        "``` python\n",
        "모델을 만들때 PyTorch는 동적 계산 그래프를 사용하기 때문에\n",
        "if / else 와 같은 조건문을 쉽게 사용할 수 있는 장점이 있습니다!\n",
        "\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "**유용한 자료**\n",
        "- [Can someone explain the use of a dynamic graph? - Reddit](https://www.reddit.com/r/pytorch/comments/8kpsjy/can_someone_explain_the_use_of_a_dynamic_graph/)"
      ],
      "metadata": {
        "id": "XVi3t4EZN0R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# TODO : 다음의 모듈(Module)을 읽고 이해해보세요!\n",
        "class Add(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.value\n",
        "\n",
        "class Sub(nn.Module):\n",
        "    def __init__(self, value):\n",
        "        super().__init__()\n",
        "        self.value = value\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x - self.value\n",
        "\n",
        "\n",
        "# TODO : Calculator 모델을 완성하세요!\n",
        "class Calculator(nn.Module):\n",
        "    def __init__(self, cal_type):\n",
        "        super().__init__()\n",
        "        self.cal_type = cal_type\n",
        "        self.add = Add(3)\n",
        "        self.sub = Sub(3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO : cal_type에 \"add\"가 입력되면 더하기 모델 y = x + 3\n",
        "        #                   \"sub\"가 입력되면 빼기 모델 y = x - 3\n",
        "        #                   \"add\", \"sub\"가 아닌 다른 문자열이 입력되면 ValueError을 일으키세요!\n",
        "        #        if/elif/else 조건문을 사용하세요! \n",
        "        if self.cal_type == 'add':  # self.cal_type, 초기 생성자의 내용을 참조해야 한다. self 사용에 유의.\n",
        "          x = self.add(x)\n",
        "        elif self.cal_type == 'sub':\n",
        "          x = self.sub(x)\n",
        "        else:\n",
        "          raise ValueError\n",
        "\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.tensor([5])\n",
        "\n",
        "try:\n",
        "    calculator = Calculator(\"none\")\n",
        "    output = calculator(x)\n",
        "\n",
        "    print(\"잘못된 문자열 입력에는 에러를 발생시키세요!!\")\n",
        "except ValueError:\n",
        "    calculator = Calculator(\"add\")\n",
        "    add_output = calculator(x)\n",
        "\n",
        "    calculator = Calculator(\"sub\")\n",
        "    sub_output = calculator(x)\n",
        "    \n",
        "    if add_output == 8 and sub_output == 2:\n",
        "        print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "    else:\n",
        "        print(\"다시 도전해봐요!\")\n",
        "except:\n",
        "    print(\"ValueError를 발생시키세요!!\")\n"
      ],
      "metadata": {
        "id": "V6Vepz3iLKLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cbc033-7b1e-4164-cc47-a61000bbd294"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter\n",
        "\n",
        "``` python\n",
        "linear transformation인 Y = XW + b 에 대해서 생각하고 있었어요!\n",
        "X는 저희가 torch.Tensor로 만들어서 제공하는데 W, b는 어디서 만들죠?\n",
        "\n",
        "언뜻 친구한테 들었는데 nn.Module안에 미리 만들어진 tensor들을\n",
        "보관할 수 있다고 들은 것 같아요! 뭐랬지, 아마 Parameter라고 한 것 같아요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.parameter.Parameter - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html?highlight=parameter)\n",
        "\n",
        "**힌트**\n",
        "- [PyTorch linear.py L81 - L85 - PyTorch 공식 Github](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L81-L85)"
      ],
      "metadata": {
        "id": "28imVTVYOOoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# TODO : Linear 모델을 완성하세요!\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO : W, b parameter를 생성하세요! 모두 1로 초기화해주세요!\n",
        "        self.W = Parameter(torch.ones([out_features, in_features]))\n",
        "        self.b = Parameter(torch.ones([out_features]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)  # WX+ b\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "x = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "linear = Linear(2, 3)\n",
        "output = linear(x)\n",
        "\n",
        "\n",
        "output \n",
        "#output == torch.Tensor([[4, 4, 4],\n",
        "                     # [8, 8, 8]])):"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvRqLdCEOS3e",
        "outputId": "ca6150ee-1205-4d4f-a2a8-20b30740300a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[4., 4., 4.],\n",
              "        [8., 8., 8.]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array([[1,2],[3,4]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUzpb4npEiJf",
        "outputId": "28cf288a-995f-4f79-f103-c13302822c0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [3, 4]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.dot(np.array([[1,2],[3,4]]),np.ones((2,3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQXmIzDQECNk",
        "outputId": "b97dd9c2-b061-40bf-f841-238f6d8f9350"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3., 3., 3.],\n",
              "       [7., 7., 7.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor vs Parameter\n",
        "``` python\n",
        "생각해보면 W, b도 tensor를 이용하면 되는 것 아닌가요?\n",
        "왜 굳이 Parameter라는 별개의 클래스를 사용하는 거죠?\n",
        "```\n",
        "**힌트**\n",
        "- 아래에 작성된 코드를 실행시키시면 힌트를 얻을 수 있습니다!"
      ],
      "metadata": {
        "id": "miKoOICOOsy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class Linear_Parameter(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.nn.parameter.Parameter\n",
        "        self.W = Parameter(torch.ones((out_features, in_features)))\n",
        "        self.b = Parameter(torch.ones(out_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Linear_Tensor(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # torch.Tensor\n",
        "        self.W = torch.ones((out_features, in_features))\n",
        "        self.b = torch.ones(out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = torch.addmm(self.b, x, self.W.T)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "x = torch.Tensor([[1, 2],\n",
        "                  [3, 4]])\n",
        "\n",
        "linear_parameter = Linear_Parameter(2, 3)\n",
        "linear_tensor = Linear_Tensor(2, 3)\n",
        "\n",
        "output_parameter = linear_parameter(x)\n",
        "output_tensor = linear_tensor(x)\n",
        "\n",
        "# 값은 동일하게 계산되는 것을 볼 수 있습니다!\n",
        "# 하지만 출력을 자세히 보시면, Parameter를 이용해서 W, b를 만들 경우에만\n",
        "# output tensor에 gradient를 계산하는 함수인 grad_fn가 생성됩니다\n",
        "print(output_parameter)\n",
        "print(output_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egvE-TfUOlfx",
        "outputId": "b4f0cbb9-5159-4c0f-960f-27e3f528e62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4., 4., 4.],\n",
            "        [8., 8., 8.]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[4., 4., 4.],\n",
            "        [8., 8., 8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameter로 만든 W, b는 저장할 tensor로 지정되어있습니다\n",
        "linear_parameter.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIxOC5acOykT",
        "outputId": "9cdab7ca-3ea8-4a8e-d5c4-264d24c89d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('W', tensor([[1., 1.],\n",
              "                      [1., 1.],\n",
              "                      [1., 1.]])), ('b', tensor([1., 1., 1.]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.Tensor로 만든 W, b는 저장되지 않습니다\n",
        "linear_tensor.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqaoxlxlOz4f",
        "outputId": "67606b59-ba3b-4937-b5c6-c28002ebeec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict()"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# TODO : 맞고 틀리고가 없는 문제입니다. 문서를 읽고 답을 자유로이 적어주세요\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "nC-6jsT7O19r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Buffer\n",
        "\n",
        "``` python\n",
        "Custom 모델을 만들때 대부분 torch.nn에 구현된 layer들을 가져가다 사용하기 때문에 Parameter를 직접 다뤄볼 일은 매우 드물어요!\n",
        "\n",
        "저희가 직접 새로운 layer를 작성할게 아니라면 Parameter를 사용할 일이 거의 없습니다!\n",
        "\n",
        "하지만 Parameter를 사용할줄 아는 것은 중요하다면서 칭찬해줬어요!\n",
        "추가적으로 buffer라는 것도 있다면서 가르켜주었죠!\n",
        "\n",
        "일반적인 Tensor는 Parameter와 다르게 gradient를 계산하지 않아\n",
        "값도 업데이트 되지 않고, 모델을 저장할 때 무시되잖아요?\n",
        "\n",
        "하지만 Parameter로 지정하지 않아서 값이 업데이트 되지 않는다 해도\n",
        "저장하고싶은 tensor가 있을 수도 있잖아요?\n",
        "\n",
        "그럴때는 buffer에 tensor를 등록해주면 되요!\n",
        "모델을 저장할때 Parameter뿐만 아니라 buffer로 등록된 tensor들도 같이 저장되요!\n",
        "\n",
        "정리하면 다음과 같아요!\n",
        "\n",
        "- \"Tensor\"\n",
        "    - ❌ gradient 계산\n",
        "    - ❌ 값 업데이트\n",
        "    - ❌ 모델 저장시 값 저장\n",
        "- \"Parameter\"\n",
        "    - ✅ gradient 계산\n",
        "    - ✅ 값 업데이트\n",
        "    - ✅ 모델 저장시 값 저장\n",
        "- \"Buffer\"\n",
        "    - ❌ gradient 계산\n",
        "    - ❌ 값 업데이트\n",
        "    - ✅ 모델 저장시 값 저장\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [register_buffer - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_buffer#torch.nn.Module.register_buffer)\n",
        "\n",
        "**유용한 자료**\n",
        "- [What is the difference between `register_buffer` and `register_parameter` of `nn.Module` - PyTorch Forum](https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723)"
      ],
      "metadata": {
        "id": "rj0LvMpbPWb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# TODO : Model 모델을 완성하세요!\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.parameter = Parameter(torch.Tensor([7]))\n",
        "        self.tensor = torch.Tensor([7])\n",
        "\n",
        "        # TODO : torch.Tensor([7])를 buffer이라는 이름으로 buffer에 등록해보세요!\n",
        "        self.register_buffer(None, None, persistent=True)\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model = Model()\n",
        "\n",
        "try:\n",
        "    buffer = model.get_buffer('buffer')\n",
        "    if buffer == 7:\n",
        "        print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\\n\")\n",
        "        print(\"🎉 이제 buffer에 등록된 tensor는 모델이 저장될 때 같이 저장될거예요! 🎉\")\n",
        "        print(model.state_dict())\n",
        "    else:\n",
        "        print(\"다시 도전해봐요!\")\n",
        "except:\n",
        "    print(\"다시 도전해봐요!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc_GFvzhO0rq",
        "outputId": "3c754e24-4a72-4014-81af-40bede4f564b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n",
            "\n",
            "🎉 이제 buffer에 등록된 tensor는 모델이 저장될 때 같이 저장될거예요! 🎉\n",
            "OrderedDict([('parameter', tensor([7.])), ('buffer', tensor([7.]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "아 너무 어려워요... 그래서 이걸 어디다 쓰는데요..?\n",
        "```\n",
        "``` python\n",
        "한가지 좋은 예시로 BatchNorm에서 사용되요!\n",
        "아래 링크를 첨부해놓았으니 더 알고 싶으면 읽어보세요!\n",
        "\n",
        "이 buffer도 Parameter와 마찬가지로 사용할 일은 드물 것 같아요!\n",
        "하지만 알아두면 언젠가 요긴하게 쓸 날이 오겠죠?\n",
        "```\n",
        "\n",
        "\n",
        "**유용한 자료**\n",
        "- [torch.nn.BatchNorm1d - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html?highlight=buffer)\n",
        "- [PyTorch batchnorm.py L51 - L52 - PyTorch 공식 Github](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/batchnorm.py#L51-L52)"
      ],
      "metadata": {
        "id": "lTdZS-joPyiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 분석해보기\n",
        "\n",
        "``` python\n",
        "이제 직접 모델을 만들어낼 줄 알게 되었습니다!\n",
        "\n",
        "모델을 만들고 난 후 어떤 module과 parameter를 썼는지\n",
        "어떻게 알 수 있는 거죠?\n",
        "\n",
        "느낌상 \"nn.Module\" Documentation에 그 방법이 나와있을 것 같아요!\n",
        "\n",
        "제가 전에 작성해놓았던 코드를 가져왔어요! 그때 보셨을 때와 조금 다르죠?\n",
        "배운 것을 적용해보느라고 조금씩 수정을 해보았어요!\n",
        "제가 만든 이 모델을 함께 분석해봐요!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.Module - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)"
      ],
      "metadata": {
        "id": "dLFJ2LI0Q-GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "# 하지만 아래 과제를 진행하기 전에 아래 코드를 보면서 최대한 이해해보세요!\n",
        "\n",
        "# Function\n",
        "class Function_A(nn.Module):\n",
        "    def __init__(self, name):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * 2\n",
        "        return x\n",
        "\n",
        "class Function_B(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([10]))\n",
        "        self.W2 = Parameter(torch.Tensor([2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x / self.W1\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "class Function_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.register_buffer('duck', torch.Tensor([7]), persistent=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * self.duck\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Function_D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.W1 = Parameter(torch.Tensor([3]))\n",
        "        self.W2 = Parameter(torch.Tensor([5]))\n",
        "        self.c = Function_C()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.W1\n",
        "        x = self.c(x)\n",
        "        x = x / self.W2\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Layer\n",
        "class Layer_AB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.a = Function_A('duck')\n",
        "        self.b = Function_B()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.a(x) / 5\n",
        "        x = self.b(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Layer_CD(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.c = Function_C()\n",
        "        self.d = Function_D()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c(x)\n",
        "        x = self.d(x) + 1\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ab = Layer_AB()\n",
        "        self.cd = Layer_CD()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ab(x)\n",
        "        x = self.cd(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "x = torch.tensor([7])\n",
        "\n",
        "model = Model()\n",
        "model(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkQNmhSwPr9s",
        "outputId": "288247a2-9dee-4204-8ef8-2eb88139c970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([6.5720], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### named_children vs named_modules\n",
        "\n",
        "``` python\n",
        "제가 만든 모델에서 어떤 module들이 있었는지 기억이 나질 않아요!\n",
        "그래서 모델 내부의 module들 목록을 보고싶어요!\n",
        "\n",
        "Documentation을 찾아보니까 children이나 module이라는 이름을 가진\n",
        "함수가 바로 제가 원하는 기능을 가진 것 같아요!\n",
        "\n",
        "하지만 이 둘은 무슨 차이일까요?\n",
        "역시 직접 코드를 통해서 확인해봐야겠어요!\n",
        "```\n",
        "\n",
        "- [named_children - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=child#torch.nn.Module.named_children)\n",
        "- [named_modules - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named#torch.nn.Module.named_modules)\n"
      ],
      "metadata": {
        "id": "6EYO8RIsRj8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(f\"[ Name ] : {name}\\n[ Module ]\\n{module}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ohS7wOZRh60",
        "outputId": "6d98e7ca-b8aa-4999-b4a6-33a371299f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Name ] : \n",
            "[ Module ]\n",
            "Model(\n",
            "  (ab): Layer_AB(\n",
            "    (a): Function_A()\n",
            "    (b): Function_B()\n",
            "  )\n",
            "  (cd): Layer_CD(\n",
            "    (c): Function_C()\n",
            "    (d): Function_D(\n",
            "      (c): Function_C()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : ab\n",
            "[ Module ]\n",
            "Layer_AB(\n",
            "  (a): Function_A()\n",
            "  (b): Function_B()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : ab.a\n",
            "[ Module ]\n",
            "Function_A()\n",
            "------------------------------\n",
            "[ Name ] : ab.b\n",
            "[ Module ]\n",
            "Function_B()\n",
            "------------------------------\n",
            "[ Name ] : cd\n",
            "[ Module ]\n",
            "Layer_CD(\n",
            "  (c): Function_C()\n",
            "  (d): Function_D(\n",
            "    (c): Function_C()\n",
            "  )\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd.c\n",
            "[ Module ]\n",
            "Function_C()\n",
            "------------------------------\n",
            "[ Name ] : cd.d\n",
            "[ Module ]\n",
            "Function_D(\n",
            "  (c): Function_C()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd.d.c\n",
            "[ Module ]\n",
            "Function_C()\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, child in model.named_children():\n",
        "    print(f\"[ Name ] : {name}\\n[ Children ]\\n{child}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwhMEddcRqCD",
        "outputId": "0daf93e9-b7e9-4d61-ea17-bfa15c986ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Name ] : ab\n",
            "[ Children ]\n",
            "Layer_AB(\n",
            "  (a): Function_A()\n",
            "  (b): Function_B()\n",
            ")\n",
            "------------------------------\n",
            "[ Name ] : cd\n",
            "[ Children ]\n",
            "Layer_CD(\n",
            "  (c): Function_C()\n",
            "  (d): Function_D(\n",
            "    (c): Function_C()\n",
            "  )\n",
            ")\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "아하! 이제 알겠어요!\n",
        "\n",
        "\"children\"은 한 단계 아래의 submodule까지만 표시하는 것이고\n",
        "\"modules\"는 자신에게 속하는 모든 submodule들을 표시해주는 것이군요!\n",
        "\n",
        "\"named_modules\", \"named_children\"은 module의 이름도 돌려주는데\n",
        "그냥 module만 필요한 경우는 \"modules\", \"children\"를 사용하면 되겠네요!\n",
        "\n",
        "아니 그런데 왜 \"Function_D\"가 \"Function_C\"를 참조하는 중이죠?\n",
        "같은 기본 단위는 서로 독립적이어야 하는데 이러면 문제가 있을 것 같아요!\n",
        "분석이 끝나면 수정을 해야겠어요!\n",
        "```"
      ],
      "metadata": {
        "id": "wy74fiwDRrlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get_submodule\n",
        "\n",
        "``` python\n",
        "제가 만든 모델 내부에 어떤 module들이 있는지 잘 알겠어요!\n",
        "이제 제가 원하는 특정 module만을 가져오고 싶어요!\n",
        "\n",
        "Function_A를 가져다 줄 수 있을까요?\n",
        "```\n",
        "\n",
        "- [get_submodule - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get_submodule#torch.nn.Module.get_submodule)\n"
      ],
      "metadata": {
        "id": "JesQb-tab8ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Function_A 를 가져오세요!\n",
        "submodule = model.None\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "if submodule.__class__.__name__  == 'Function_A':\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"다시 도전해봐요!\")"
      ],
      "metadata": {
        "id": "_5Dtffz2RqLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter\n",
        "\n",
        "``` python\n",
        "이제 module에 대한 자신감이 생겼어요!\n",
        "그런데 문득 생각해보니까 Parameter에 대해서 공부하면서\n",
        "제가 어떤 모듈에 Parameter를 생성해놨던 것 같아요!\n",
        "\n",
        "저희가 module에서 module 목록을 보고 특정 module을\n",
        "찾기도 했던 것처럼 Parameter에서도 똑같이 해봐요!\n",
        "```\n",
        "\n",
        "- [parameters - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=parameters#torch.nn.Module.parameters)\n",
        "- [named_parameters - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named#torch.nn.Module.named_parameters)\n",
        "- [get_parameter - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get#torch.nn.Module.get_parameter)\n"
      ],
      "metadata": {
        "id": "DuR9nL2NcED8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4개의 Parameter를 만들었었군요!\n",
        "for name, parameter in model.named_parameters():\n",
        "    print(f\"[ Name ] : {name}\\n[ Parameter ]\\n{parameter}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G42FBHYUcFpV",
        "outputId": "63fdfbfa-191b-41f7-9518-7b5848aa0830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Name ] : ab.b.W1\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([10.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : ab.b.W2\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([2.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : cd.d.W1\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([3.], requires_grad=True)\n",
            "------------------------------\n",
            "[ Name ] : cd.d.W2\n",
            "[ Parameter ]\n",
            "Parameter containing:\n",
            "tensor([5.], requires_grad=True)\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "``` python\n",
        "아하! 제가 4개의 Parameter를 만들었었네요!\n",
        "이름을 표시하니까 Parameter가 어디에 속하는지 알 수 있어 너무 좋아요!\n",
        "\n",
        "- \"Function_B\"에 W1, W2 Parameter 2개\n",
        "- \"Function_D\"에 W1, W2 Parameter 2개\n",
        "\n",
        "\"parameters\"를 사용해서 목록을 확인해도 되지만\n",
        "이름이 표시 안되니까 어떤 module에 속한\n",
        "Parameter인지 알기가 너무 힘들 것 같아요!\n",
        "\n",
        "Function_B에 속해있는 Parameter W1을 사용하고 싶은데\n",
        "이를 가져다 주실수 있나요?\n",
        "```"
      ],
      "metadata": {
        "id": "aMUMfuS2cJpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Function_B에 속하는 Parameter W1을 가져오세요!\n",
        "parameter = model.None\n",
        "\n",
        "\n",
        "parameter # 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NVopWi_cHlL",
        "outputId": "6f0a09f2-13e0-4326-df90-af983d9b8385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([10.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buffer\n",
        "\n",
        "``` python\n",
        "생각을 해보니까 buffer도 하나 추가해뒀던 것 같아요!\n",
        "buffer도 같이 분석해봐요!\n",
        "```\n",
        "\n",
        "- [buffers - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=buffers#torch.nn.Module.buffers)\n",
        "- [named_buffers - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=named_buffers#torch.nn.Module.named_buffers)\n",
        "- [get_buffer - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=get_buffer#torch.nn.Module.get_buffer)\n"
      ],
      "metadata": {
        "id": "EDLs3CwscYTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : named_buffers를 사용해서 model에 속하는 buffer 전체 목록을 가져오세요!\n",
        "for name, buffer in model.None:\n",
        "    print(f\"[ Name ] : {name}\\n[ Buffer ] : {buffer}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTRdv5sHcQ0Z",
        "outputId": "2c369573-cf1c-435e-ca09-765ba3992476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Name ] : cd.c.duck\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n",
            "[ Name ] : cd.d.c.duck\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : buffers를 사용해서 model에 속하는 buffer 전체 목록을 가져오세요!\n",
        "for buffer in model.None:\n",
        "    print(f\"[ Buffer ] : {buffer}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cjgP3gmcbTr",
        "outputId": "df2f6ec5-ac63-40d5-ce85-e7c23ce29df0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n",
            "[ Buffer ] : tensor([7.])\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Function_C에 속하는 Buffer를 가져오세요!\n",
        "buffer = model.None\n",
        "\n",
        "\n",
        "buffer # 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6hzkyXXccWF",
        "outputId": "5051923f-7a00-464b-d7ae-b9ae1fe4edbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7.])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 수정하기 - Docstring 작성\n",
        "\n",
        "``` python\n",
        "가볍게 Docstring을 작성해봐요!\n",
        "Numpy, Pydoc, Google 등 다양한 스타일이 있으니까 궁금하면\n",
        "아래 링크에서 읽어보면서 공부해보도록 해요!\n",
        "\n",
        "지금은 스타일 상관없이 Docstring이라는 것을 추가하기만 해보죠!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "\n",
        "**✨ 유용한 자료 ✨**\n",
        "- [Docstrings in Python - Data Camp](https://www.datacamp.com/community/tutorials/docstrings-python)"
      ],
      "metadata": {
        "id": "fLnnpVbTc6LC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "# TODO : Docstring을 추가해보세요!\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "model = Model()\n",
        "\n",
        "if model.__doc__:\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"다시 도전해봐요!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eIpgLsIcdpG",
        "outputId": "1d1dc3d0-b620-435f-a068-10737c02f748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BatchNorm1d 분석해보기\n",
        "\n",
        "``` python\n",
        "PyTorch가 미리 만들어둔 module중 하나인 BatchNorm1d로 분석해봅시다!\n",
        "```\n",
        "\n",
        "- [Documentation main - PyTorch 공식 문서](https://pytorch.org/docs/stable/index.html)\n",
        "- [torch.nn.BatchNorm1d - PyTorch 공식 문서](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)"
      ],
      "metadata": {
        "id": "aJakKeGudCT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "module = nn.BatchNorm1d(10)\n",
        "\n",
        "# TODO : nn.BatchNorm1d의 parameter와 buffer 갯수를 알아내세요!\n",
        "parameter_n = len(list(module.None))\n",
        "buffer_n = len(list(module.None))\n",
        "\n",
        "# TODO : nn.BatchNorm1d의 buffer 이름을 알아내세요!\n",
        "#        [이름, 이름, 이름] 형태로 저장해주세요!\n",
        "buffer_names = [name for name, _ in module.named_buffers()]\n",
        "\n",
        "\n",
        "\n",
        "# 아래 코드는 수정하실 필요가 없습니다!\n",
        "answer = set(['running_mean', 'running_var', 'num_batches_tracked'])\n",
        "\n",
        "if parameter_n == 2 and buffer_n == 3 and answer == set(buffer_names):\n",
        "    print(\"🎉🎉🎉 성공!!! 🎉🎉🎉\")\n",
        "else:\n",
        "    print(\"다시 도전해봐요!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cyf79gcc-8-",
        "outputId": "6bd97ae4-d320-49e9-fc78-3cdcdfd0e160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉🎉🎉 성공!!! 🎉🎉🎉\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlCg-tVvdOgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}