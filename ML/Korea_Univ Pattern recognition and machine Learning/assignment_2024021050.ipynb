{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE Estiamation \n",
    "\n",
    "$w = (X^T\\cdot X)^{-1}\\cdot X^{T}\\cdot y$, because the cost function $||Xw - y||^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP Estimation Overview\n",
    "MAP estimation in regression involves incorporating a prior belief about the distribution of model parameters (in this case, the polynomial coefficients) and updating this belief based on the observed data. \n",
    "\n",
    "For polynomial regression, if we assume a Gaussian prior for the coefficients, the MAP estimation modifies the objective function used in the regression to include a regularization term that reflects this prior.\n",
    "\n",
    "## Key Components of MAP in Polynomial Regression:\n",
    "\n",
    "#### 1. Prior Distribution: You have a Gaussian prior on the polynomial coefficients ğ‘¤, typically expressed as:\n",
    "\n",
    "$$ğ‘(ğ‘¤âˆ£ğ›¼)=ğ‘(0,ğ›¼^{âˆ’1}ğ¼)$$\n",
    "\n",
    "Here, $ğ›¼$ is the precision of the prior (not the variance). It reflects how strongly you believe the coefficients should be close to zero (larger ğ›¼ implies stronger belief in smaller magnitude weights).\n",
    "\n",
    "#### 2. Likelihood Function: This is the likelihood of observing the data given the model parameters. \n",
    "\n",
    "In linear regression, assuming Gaussian noise, the likelihood for a given set of parameters ğ‘¤ is also Gaussian centered around the model predictions. \n",
    "\n",
    "#### 3. Posterior Distribution: MAP estimates the coefficients ğ‘¤ by maximizing the posterior distribution, which, due to Bayes' Rule, is proportional to the product of the likelihood and the prior:\n",
    "\n",
    "$$ğ‘(ğ‘¤âˆ£ğ‘¡,ğ‘‹,ğ›¼,ğ›½)âˆğ‘(ğ‘¡âˆ£ğ‘‹,ğ‘¤,ğ›½)Ã—ğ‘(ğ‘¤âˆ£ğ›¼)p(wâˆ£t,X,Î±,Î²)âˆp(tâˆ£X,w,Î²)Ã—p(wâˆ£Î±)$$ \n",
    "\n",
    "where ğ›½ is the precision of the likelihood (related to the noise variance).\n",
    "\n",
    "#### 4. Regularized Objective Function: In practical terms, maximizing the posterior is equivalent to minimizing a loss function that adds a regularization term to the usual sum of squared residuals:\n",
    "\n",
    "$$Minimize: âˆ£âˆ£ğ‘‹ğ‘¤âˆ’ğ‘¡âˆ£âˆ£^{2}+ğ›¼âˆ£âˆ£ğ‘¤âˆ£âˆ£^{2}$$\n",
    " \n",
    "The regularization term $ğ›¼âˆ£âˆ£ğ‘¤âˆ£âˆ£^{2}$ comes directly from the log of the Gaussian prior and penalizes large coefficients, effectively controlling overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for Bayesian Regression\n",
    "\n",
    "### 1. Prior Distribution: \n",
    "\n",
    "Assume a Gaussian prior for the weights ğ‘¤ similar to MAP, \n",
    "\n",
    "$$ğ‘(ğ‘¤âˆ£ğ›¼)=ğ‘(0,ğ›¼^{âˆ’1}ğ¼)$$\n",
    "\n",
    "### 2. Likelihood: \n",
    "\n",
    "The likelihood remains Gaussian centered around the model predictions, \n",
    "\n",
    "$$p(tâˆ£X,w,Î²)$$\n",
    "\n",
    "### 3. Posterior Distribution: \n",
    "\n",
    "Compute the posterior distribution over the weights, \n",
    "\n",
    "$$p(wâˆ£t,X,Î±,Î²)$$\n",
    "\n",
    "which is also Gaussian but with updated mean and covariance based on the data:\n",
    "\n",
    "$$ğ‘¤âˆ£ğ‘¡,Xâˆ¼N(m_{N} ,S_{N})$$\n",
    "\n",
    "where $ğ‘š_{ğ‘}$ and $S_{N}$ are updated based on the data and the priors.\n",
    "\n",
    "### 4. Predictive Distribution: \n",
    "\n",
    "For new input $ğ‘¥^{âˆ—}$, the predictive distribution of the target $ğ‘¡^{âˆ—}$ is also Gaussian:\n",
    "\n",
    "$$ğ‘¡^{âˆ—}âˆ£ğ‘¥^{âˆ—},ğ‘¡,ğ‘‹âˆ¼ğ‘(ğ‘š_{ğ‘}^{ğ‘‡} ğœ™(x^{âˆ—}),ğœ_{ğ‘}^{2}(ğ‘¥^{âˆ—}))$$\n",
    "\n",
    "where $ğœ™(^{âˆ—})$ are the polynomial features of $x^{âˆ—}$ and $ğœ_{ğ‘}^{2}(ğ‘¥^{âˆ—})$ includes the variance from the model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import normal, uniform\n",
    "from scipy.stats import multivariate_normal as mv_norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Set the random seed\n",
    "np.random.seed(0)\n",
    "## Function to generate polynomial features\n",
    "def polynomial_features(x, order):\n",
    "    \"\"\" Generate polynomial features up to the given order \"\"\"\n",
    "    features = np.vander(x, N=order+1, increasing=True)\n",
    "    return features\n",
    "\n",
    "## Function to perform MLE Regression\n",
    "def mle_regression(x, t, order):\n",
    "    \"\"\" Perform MLE regression using polynomial features, when you calculate the w, the prior distribution is not considered \"\"\"\n",
    "    ## Generate polynomial features\n",
    "    X = polynomial_features(x, order)\n",
    "    ## Compute the weights using the normal equation\n",
    "    w_mle = np.linalg.inv(X.T @ X) @ X.T @ t  ## w = (X^T*X)^(-1)*X^(T)*t, because the cost function ||Xw - y||^2 \n",
    "    return w_mle\n",
    "\n",
    "def map_regression(x, t, order, alpha):\n",
    "    \"\"\" Perform MAP regression using polynomial features \"\"\"\n",
    "    X = polynomial_features(x, order)\n",
    "    ## Adjust the normal equation to include the regularization term\n",
    "    regularized_matrix = X.T @ X + alpha * np.eye(order + 1)\n",
    "    w_map = np.linalg.inv(regularized_matrix) @ X.T @ t\n",
    "    return w_map\n",
    "\n",
    "## Function to predict new values\n",
    "def mle_predict(x, w):\n",
    "    \"\"\" Predict t using the MLE model \"\"\"\n",
    "    X_new = polynomial_features(x, ORDER)\n",
    "    t_pred = X_new @ w\n",
    "    return t_pred\n",
    "\n",
    "# Bayesian Regression Function\n",
    "def bayesian_regression(x, t, order, alpha, beta):\n",
    "    \"\"\" Perform Bayesian regression \"\"\"\n",
    "    X = polynomial_features(x, order)\n",
    "    S_N_inv = alpha * np.eye(order + 1) + beta * (X.T @ X)\n",
    "    S_N = np.linalg.inv(S_N_inv)\n",
    "    m_N = beta * S_N @ X.T @ t\n",
    "    return m_N, S_N\n",
    "\n",
    "# Predictive Distribution\n",
    "def predictive_distribution(x_new, m_N, S_N, beta):\n",
    "    \"\"\" Calculate predictive mean and variance \"\"\"\n",
    "    phi_new = polynomial_features(x_new, ORDER)\n",
    "    mean = phi_new @ m_N\n",
    "    variance = 1 / beta + np.sum(phi_new @ S_N * phi_new, axis=1)\n",
    "    return mean, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants\n",
    "BETA = 11.1 ## Beta is a precision for the likelihood function\n",
    "ALPHA = 5e-3 ## alpha is a precision for prior distribution\n",
    "NUM_SAMPLES = 100\n",
    "ORDER = 9\n",
    "\n",
    "## Generate sample points: using random sampling instead of linspace to get i.i.d samples\n",
    "np.random.seed(0)  ## For reproducibility\n",
    "x_samples = np.random.uniform(-1, 1, NUM_SAMPLES)\n",
    "## Calculating the target values t with noise\n",
    "t_samples = np.cos(2 * np.pi * x_samples) + np.sin(np.pi * x_samples) + np.random.normal(0, 1 / np.sqrt(BETA), NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit MLE model\n",
    "w_mle = mle_regression(x_samples, t_samples, ORDER)\n",
    "## Fit MAP model\n",
    "w_map = map_regression(x_samples, t_samples, ORDER, ALPHA)\n",
    "## Bayesian Regression\n",
    "m_N, S_N = bayesian_regression(x_samples, t_samples, ORDER, ALPHA, BETA)\n",
    "## Test the model with new samples\n",
    "x_test = np.linspace(-1, 1, 100)\n",
    "means, variances = predictive_distribution(x_test, m_N, S_N, BETA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì°¸ê³ í•œ ì‚¬ì´íŠ¸\n",
    "\n",
    "https://euphoria0-0.github.io/posts/Bayesian-Linear-Regression/\n",
    "\n",
    "https://github.com/zjost/bayesian-linear-regression/blob/master/src/bayes-regression.ipynb\n",
    "\n",
    "https://code-first-ml.github.io/book1/notebooks/introduction/mle_coin.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
