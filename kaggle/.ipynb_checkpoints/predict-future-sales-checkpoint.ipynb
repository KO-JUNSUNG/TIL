{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-09T05:19:13.480449Z",
     "iopub.status.busy": "2023-01-09T05:19:13.479544Z",
     "iopub.status.idle": "2023-01-09T05:19:13.507682Z",
     "shell.execute_reply": "2023-01-09T05:19:13.506403Z",
     "shell.execute_reply.started": "2023-01-09T05:19:13.480361Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:48:49.307545Z",
     "iopub.status.busy": "2023-01-09T06:48:49.307170Z",
     "iopub.status.idle": "2023-01-09T06:48:49.312341Z",
     "shell.execute_reply": "2023-01-09T06:48:49.311325Z",
     "shell.execute_reply.started": "2023-01-09T06:48:49.307512Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:48:53.097529Z",
     "iopub.status.busy": "2023-01-09T06:48:53.096335Z",
     "iopub.status.idle": "2023-01-09T06:48:54.173303Z",
     "shell.execute_reply": "2023-01-09T06:48:54.172332Z",
     "shell.execute_reply.started": "2023-01-09T06:48:53.097483Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH=\"/kaggle/input/competitive-data-science-predict-future-sales/\"\n",
    "train = pd.read_csv(PATH + \"sales_train.csv\")\n",
    "test = pd.read_csv(PATH + \"test.csv\")\n",
    "shop = pd.read_csv(PATH + \"shops.csv\")\n",
    "item = pd.read_csv(PATH + \"items.csv\")\n",
    "category = pd.read_csv(PATH + \"item_categories.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 노트북 내내 진행되는 간단한 유틸 기능들\n",
    "A few utility functions used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다운캐스팅은 부모 클래스가 자식 클래스 객체로 형변환 되는 것.\n",
    "\n",
    "def reduce_mem_usage(df, silent=True, allow_categorical=True, float_dtype=\"float32\"):\n",
    "    \"\"\" \n",
    "    Iterates through all the columns of a dataframe and downcasts the data type\n",
    "     to reduce memory usage. Can also factorize categorical columns to integer dtype.\n",
    "    \"\"\"\n",
    "    def _downcast_numeric(series, allow_categorical=allow_categorical):\n",
    "        \"\"\"\n",
    "        Downcast a numeric series into either the smallest possible int dtype or a specified float dtype.\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_sparse(series.dtype) is True:\n",
    "            return series\n",
    "        elif pd.api.types.is_numeric_dtype(series.dtype) is False:\n",
    "            if pd.api.types.is_datetime64_any_dtype(series.dtype):\n",
    "                return series\n",
    "            else:\n",
    "                if allow_categorical:\n",
    "                    return series\n",
    "                else:\n",
    "                    codes, uniques = series.factorize()\n",
    "                    series = pd.Series(data=codes, index=series.index)\n",
    "                    series = _downcast_numeric(series)\n",
    "                    return series\n",
    "        else:\n",
    "            series = pd.to_numeric(series, downcast=\"integer\")\n",
    "        if pd.api.types.is_float_dtype(series.dtype):\n",
    "            series = series.astype(float_dtype)\n",
    "        return series\n",
    "\n",
    "    if silent is False:\n",
    "        start_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    if df.ndim == 1:\n",
    "        df = _downcast_numeric(df)\n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            df.loc[:, col] = _downcast_numeric(df.loc[:,col])\n",
    "    if silent is False:\n",
    "        end_mem = np.sum(df.memory_usage()) / 1024 ** 2\n",
    "        print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "        print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def shrink_mem_new_cols(matrix, oldcols=None, allow_categorical=False):\n",
    "    # Calls reduce_mem_usage on columns which have not yet been optimized\n",
    "    if oldcols is not None:\n",
    "        newcols = matrix.columns.difference(oldcols)\n",
    "    else:\n",
    "        newcols = matrix.columns\n",
    "    matrix.loc[:,newcols] = reduce_mem_usage(matrix.loc[:,newcols], allow_categorical=allow_categorical)\n",
    "    oldcols = matrix.columns  # This is used to track which columns have already been downcast\n",
    "    return matrix, oldcols\n",
    "\n",
    "\n",
    "def list_if_not(s, dtype=str):\n",
    "    # Puts a variable in a list if it is not already a list\n",
    "    if type(s) not in (dtype, list):\n",
    "        raise TypeError\n",
    "    if (s != \"\") & (type(s) is not list):\n",
    "        s = [s]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:29:57.928361Z",
     "iopub.status.busy": "2023-01-09T06:29:57.927955Z",
     "iopub.status.idle": "2023-01-09T06:29:57.939493Z",
     "shell.execute_reply": "2023-01-09T06:29:57.938719Z",
     "shell.execute_reply.started": "2023-01-09T06:29:57.928332Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.head(10))\n",
    "print(\"==\"*10)\n",
    "print(test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:30:57.414241Z",
     "iopub.status.busy": "2023-01-09T06:30:57.413885Z",
     "iopub.status.idle": "2023-01-09T06:30:57.428097Z",
     "shell.execute_reply": "2023-01-09T06:30:57.427013Z",
     "shell.execute_reply.started": "2023-01-09T06:30:57.414212Z"
    }
   },
   "outputs": [],
   "source": [
    "item.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:31:11.180184Z",
     "iopub.status.busy": "2023-01-09T06:31:11.179748Z",
     "iopub.status.idle": "2023-01-09T06:31:11.189768Z",
     "shell.execute_reply": "2023-01-09T06:31:11.188712Z",
     "shell.execute_reply.started": "2023-01-09T06:31:11.180152Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"==\"*10)\n",
    "print(shop.head(10))\n",
    "print(\"==\"*10)\n",
    "print(category.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:49:06.451480Z",
     "iopub.status.busy": "2023-01-09T06:49:06.448613Z",
     "iopub.status.idle": "2023-01-09T06:49:06.457318Z",
     "shell.execute_reply": "2023-01-09T06:49:06.456323Z",
     "shell.execute_reply.started": "2023-01-09T06:49:06.451418Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:43:43.647870Z",
     "iopub.status.busy": "2023-01-09T06:43:43.647230Z",
     "iopub.status.idle": "2023-01-09T06:43:43.669710Z",
     "shell.execute_reply": "2023-01-09T06:43:43.668481Z",
     "shell.execute_reply.started": "2023-01-09T06:43:43.647831Z"
    }
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:49:10.497062Z",
     "iopub.status.busy": "2023-01-09T06:49:10.496364Z",
     "iopub.status.idle": "2023-01-09T06:49:10.614502Z",
     "shell.execute_reply": "2023-01-09T06:49:10.613329Z",
     "shell.execute_reply.started": "2023-01-09T06:49:10.497028Z"
    }
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T06:49:36.397106Z",
     "iopub.status.busy": "2023-01-09T06:49:36.396743Z",
     "iopub.status.idle": "2023-01-09T06:49:36.520942Z",
     "shell.execute_reply": "2023-01-09T06:49:36.519473Z",
     "shell.execute_reply.started": "2023-01-09T06:49:36.397076Z"
    }
   },
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- datetime 연산을 할 수 있게 date feature 을 datetime 타입 객체로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T07:20:15.026980Z",
     "iopub.status.busy": "2023-01-09T07:20:15.026302Z",
     "iopub.status.idle": "2023-01-09T07:20:15.364902Z",
     "shell.execute_reply": "2023-01-09T07:20:15.363800Z",
     "shell.execute_reply.started": "2023-01-09T07:20:15.026940Z"
    }
   },
   "outputs": [],
   "source": [
    "train[\"date\"] = pd.to_datetime(train[\"date\"], format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T07:22:26.008909Z",
     "iopub.status.busy": "2023-01-09T07:22:26.008553Z",
     "iopub.status.idle": "2023-01-09T07:22:26.137651Z",
     "shell.execute_reply": "2023-01-09T07:22:26.136308Z",
     "shell.execute_reply.started": "2023-01-09T07:22:26.008879Z"
    }
   },
   "outputs": [],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T07:22:15.897454Z",
     "iopub.status.busy": "2023-01-09T07:22:15.897020Z",
     "iopub.status.idle": "2023-01-09T07:22:15.914258Z",
     "shell.execute_reply": "2023-01-09T07:22:15.913394Z",
     "shell.execute_reply.started": "2023-01-09T07:22:15.897383Z"
    }
   },
   "outputs": [],
   "source": [
    "shop.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T07:32:09.040972Z",
     "iopub.status.busy": "2023-01-09T07:32:09.040269Z",
     "iopub.status.idle": "2023-01-09T07:32:09.052847Z",
     "shell.execute_reply": "2023-01-09T07:32:09.051820Z",
     "shell.execute_reply.started": "2023-01-09T07:32:09.040934Z"
    }
   },
   "outputs": [],
   "source": [
    "shop.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "훈련 DF 를 일반적인 데이터 정제 과정을 통해 정제함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shop의 중복 id 통합\n",
    "train[\"shop_id\"] = train[\"shop_id\"].replace({0: 57, 1: 58, 11: 10, 40: 39})\n",
    "# test 셋에 남아있는 shop_id 만 사용\n",
    "# 점수를 올리기 위한 일종의 과적합 테크닉\n",
    "train = train.loc[train.shop_id.isin(test[\"shop_id\"].unique()), :]\n",
    "# Drop training items with extreme or negative prices or sales counts\n",
    "train = train[(train[\"item_price\"] > 0) & (train[\"item_price\"] < 50000)]\n",
    "train = train[(train[\"item_cnt_day\"] > 0) & (train[\"item_cnt_day\"] < 1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "test data는 test month에 등록된 가격들로 shop 과 item의 모든 가능한 조합(데카르트 곱) 인 것으로 보입니다. 이것들의 목표 데이터는 각 shop-item 조합에 따른 총 월매출입니다. train 행렬(matrix)은 이런 구조들을 train 데이터 기간동안 똑같이 복제해 사용하는 형태입니다. feature들이 test 기간동안 만들어질 수 있도록, 테스트 item들은 training set에 마지막에 연결됩니다.   \n",
    "The test data seems to be every possible combination (the cartesian product) of shops and items that registered a sale in the test month, with the target as the total month's sales made for each of these shop-item combinations. Here a training matrix is made that replicates this structure for every month in the training data period. The test items are concatenated to the end of the training data so that features can be generated for the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 test 데이터와 병합하여 새로운 train data 만들기.\n",
    "# 점수를 올리기 위한 일종의 과적합 테크닉.\n",
    "\n",
    "def create_testlike_train(sales_train, test=None):\n",
    "    indexlist = []\n",
    "    for i in sales_train.date_block_num.unique():\n",
    "        x = itertools.product(\n",
    "            [i],\n",
    "            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n",
    "            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n",
    "        )\n",
    "        indexlist.append(np.array(list(x)))\n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(indexlist, axis=0),\n",
    "        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    # Add revenue column to sales_train\n",
    "    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_day\"]\n",
    "    # Aggregate item_id / shop_id item_cnts and revenue at the month level\n",
    "    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n",
    "        item_cnt_month=pd.NamedAgg(column=\"item_cnt_day\", aggfunc=\"sum\"),\n",
    "        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n",
    "    )\n",
    "\n",
    "    # Merge the grouped data with the index\n",
    "    df = df.merge(\n",
    "        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n",
    "    )\n",
    "\n",
    "    if test is not None:\n",
    "        test[\"date_block_num\"] = 34\n",
    "        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n",
    "        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n",
    "        test[\"item_id\"] = test.item_id.astype(np.int16)\n",
    "        test = test.drop(\"ID\",1)\n",
    "\n",
    "        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n",
    "\n",
    "    # Fill empty item_cnt entries with 0\n",
    "    df.item_cnt_month = df.item_cnt_month.fillna(0)\n",
    "    df.item_revenue_month = df.item_revenue_month.fillna(0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_testlike_train(train, test)\n",
    "del(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** 'reduce_mem_usage' 기능은 데이터타입을 다운캐스트 하는데 이는 캐글 노트북 상에서 실행되는 중 메모리 오버플로우가 일어나지 않도록 하기 위해 필수적인 작업입니다. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = reduce_mem_usage(matrix, silent=False)\n",
    "oldcols = matrix.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "이 구간에서는 예측 정확성을 위해 임의의 feature 열들이 만들어지고 정렬 속에 추가됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item name groups with fuzzywuzzy\n",
    "- item 테이블 내의 item 들은 필드 내에서 알파벳 순으로 정렬됩니다. 따라서 비슷한 아이템들은 일반적으로 근처에 배치됩니다. 예를 들어 한 개의 테이블 내의 최초의 두 개의 아이템들은 두 개의 다른 형태의 인터넷 보안을 사용하는 다른 콘솔을 사용한 같은 게임, 'Fuse'를 가리킵니다. \n",
    "- 이러한 정렬방식은 비슷한 종류의 아이템을 그룹핑하는데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.query(\"item_id>3564\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이하의 셀들은 비슷한 아이템들을 루프를 통해 연속적으로 묶는 과정입니다.\n",
    "- string 타입의 데이터들을 이름의 비슷한 정도를 fuzzywuzzy 라이브러리를 사용하여 측정하고 일치값이 임계값을 초과하는 경우 같은 그룹으로 배정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def add_item_name_groups(matrix, train, items, sim_thresh, feature_name=\"item_name_group\"):\n",
    "    def partialmatchgroups(items, sim_thresh=sim_thresh):\n",
    "        def strip_brackets(string):\n",
    "            string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "            string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "            return string\n",
    "\n",
    "        items = items.copy()\n",
    "        items[\"nc\"] = items.item_name.apply(strip_brackets)\n",
    "        items[\"ncnext\"] = np.concatenate((items[\"nc\"].to_numpy()[1:], np.array([\"\"])))\n",
    "\n",
    "        def partialcompare(s):\n",
    "            return fuzz.partial_ratio(s[\"nc\"], s[\"ncnext\"])\n",
    "\n",
    "        items[\"partialmatch\"] = items.apply(partialcompare, axis=1)\n",
    "        # Assign groups\n",
    "        grp = 0\n",
    "        for i in range(items.shape[0]):\n",
    "            items.loc[i, \"partialmatchgroup\"] = grp\n",
    "            if items.loc[i, \"partialmatch\"] < sim_thresh:\n",
    "                grp += 1\n",
    "        items = items.drop(columns=[\"nc\", \"ncnext\", \"partialmatch\"])\n",
    "        return items\n",
    "\n",
    "    items = partialmatchgroups(items)\n",
    "    items = items.rename(columns={\"partialmatchgroup\": feature_name})\n",
    "    items = items.drop(columns=\"partialmatchgroup\", errors=\"ignore\")\n",
    "\n",
    "    items[feature_name] = items[feature_name].apply(str)\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    train = train.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\")\n",
    "    return matrix, train\n",
    "\n",
    "\n",
    "matrix, train = add_item_name_groups(matrix, train, items, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music artist / first word of item name\n",
    "- 이 기능은 music item 들을 item name 에서 3가지 일반적인 패턴(전부 대문자 형태, 제목 사이에 더블스페이스로 구분한 것, .으로 구분한 것)으로 추출한 artist name 에 따라 그룹으로 분류합니다. \n",
    "\n",
    "- non-music 카테고리들을 위해서는 대신해 아이템들을 첫번째 단어에 따라 분류합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def add_first_word_features(matrix, items=items, feature_name=\"artist_name_or_first_word\"):\n",
    "    # This extracts artist names for music categories and adds them as a feature.\n",
    "    def extract_artist(st):\n",
    "        st = st.strip()\n",
    "        if st.startswith(\"V/A\"):\n",
    "            artist = \"V/A\"\n",
    "        elif st.startswith(\"СБ\"):\n",
    "            artist = \"СБ\"\n",
    "        else:\n",
    "            # Retrieves artist names using the double space or all uppercase pattern\n",
    "            mus_artist_dubspace = re.compile(r\".{2,}?(?=\\s{2,})\")\n",
    "            match_dubspace = mus_artist_dubspace.match(st)\n",
    "            mus_artist_capsonly = re.compile(r\"^([^a-zа-я]+\\s)+\")\n",
    "            match_capsonly = mus_artist_capsonly.match(st)\n",
    "            candidates = [match_dubspace, match_capsonly]\n",
    "            candidates = [m[0] for m in candidates if m is not None]\n",
    "            # Sometimes one of the patterns catches some extra words so choose the shortest one\n",
    "            if len(candidates):\n",
    "                artist = min(candidates, key=len)\n",
    "            else:\n",
    "                # If neither of the previous patterns found something, use the dot-space pattern\n",
    "                mus_artist_dotspace = re.compile(r\".{2,}?(?=\\.\\s)\")\n",
    "                match = mus_artist_dotspace.match(st)\n",
    "                if match:\n",
    "                    artist = match[0]\n",
    "                else:\n",
    "                    artist = \"\"\n",
    "        artist = artist.upper()\n",
    "        artist = re.sub(r\"[^A-ZА-Я ]||\\bTHE\\b\", \"\", artist)\n",
    "        artist = re.sub(r\"\\s{2,}\", \" \", artist)\n",
    "        artist = artist.strip()\n",
    "        return artist\n",
    "\n",
    "    items = items.copy()\n",
    "    all_stopwords = stopwords.words(\"russian\")\n",
    "    all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "    def first_word(string):\n",
    "        # This cleans the string of special characters, excess spaces and stopwords then extracts the first word\n",
    "        string = re.sub(r\"[^\\w\\s]\", \"\", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "        tokens = string.lower().split()\n",
    "        tokens = [t for t in tokens if t not in all_stopwords]\n",
    "        token = tokens[0] if len(tokens) > 0 else \"\"\n",
    "        return token\n",
    "\n",
    "    music_categories = [55, 56, 57, 58, 59, 60]\n",
    "    items.loc[items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(extract_artist)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other music\"\n",
    "    items.loc[~items.item_category_id.isin(music_categories), feature_name] = items.loc[\n",
    "        ~items.item_category_id.isin(music_categories), \"item_name\"\n",
    "    ].apply(first_word)\n",
    "    items.loc[items[feature_name] == \"\", feature_name] = \"other non-music\"\n",
    "    items[feature_name] = items[feature_name].factorize()[0]\n",
    "    matrix = matrix.merge(items[[\"item_id\", feature_name]], on=\"item_id\", how=\"left\",)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = add_first_word_features(\n",
    "    matrix, items=items, feature_name=\"artist_name_or_first_word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아이템 이름 길이를 특징(feature)으로 변환\n",
    "- 아이템 이름 길이 특징은 매우 성능이 좋습니다. 아마도 비슷한 아이템들은 비슷한 이름 길이를 가지고 있기 때문일 것입니다. \n",
    "- 이 방법은 그냥 데이터와 정제된 데이터 모두에서 뛰어난 효과를 발휘했습니다. 이 방법은 항목간의 애매한 유사성들에 대한 정보를 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_item_name(string):\n",
    "    # 괄호로 묶여있는 용어들 및 특수문자와 공백을 제거합니다.\n",
    "    #\n",
    "    string = re.sub(r\"\\[.*?\\]\", \"\", string)\n",
    "    string = re.sub(r\"\\(.*?\\)\", \"\", string)\n",
    "    string = re.sub(r\"[^A-ZА-Яa-zа-я0-9 ]\", \"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "\n",
    "items[\"item_name_cleaned_length\"] = items[\"item_name\"].apply(clean_item_name).apply(len)\n",
    "items[\"item_name_length\"] = items[\"item_name\"].apply(len)\n",
    "matrix = matrix.merge(items[['item_id', 'item_name_length', 'item_name_cleaned_length']], how='left', on='item_id')\n",
    "items = items.drop(columns=['item_name_length', 'item_name_cleaned_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created name features\")\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time features\n",
    "- 일자와 월별 정확도(resolution) time feature 들은 training 데이터프레임에서 만들어졌습니다. 예를 들어 각 품목이 팔린 첫날과 마지막날 같은 데이터 말입니다.\n",
    "- 각 항목의 첫번째 판매 이후의 시간은 sales-per-day 의 평균을 나타내는 feature(= (\"item_cnt_day_avg\") column)을 만드는데 사용됩니다. \n",
    "- 이 항목은 한 달이 지나기 전에 팔렸거나 해서 이전 달 전체 기간 내내 동안은 살 수 없었던 데이터들을 정정하는데 도움이 됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(m, train, correct_item_cnt_day=False):\n",
    "    from pandas.tseries.offsets import Day, MonthBegin, MonthEnd\n",
    "\n",
    "    def item_shop_age_months(m):\n",
    "        m[\"item_age\"] = m.groupby(\"item_id\")[\"date_block_num\"].transform(\n",
    "            lambda x: x - x.min()\n",
    "        )\n",
    "        # Sales tend to plateau after 12 months\n",
    "        m[\"new_item\"] = m[\"item_age\"] == 0\n",
    "        m[\"new_item\"] = m[\"new_item\"].astype(\"int8\")\n",
    "        m[\"shop_age\"] = (\n",
    "            m.groupby(\"shop_id\")[\"date_block_num\"]\n",
    "            .transform(lambda x: x - x.min())\n",
    "            .astype(\"int8\")\n",
    "        )\n",
    "        return m\n",
    "\n",
    "    # Add dummy values for the test month so that features are created correctly\n",
    "    dummies = m.loc[m.date_block_num == 34, [\"date_block_num\", \"shop_id\", \"item_id\"]]\n",
    "    dummies = dummies.assign(\n",
    "        date=pd.to_datetime(\"2015-11-30\"), item_price=1, item_cnt_day=0, item_revenue_day=0,\n",
    "    )\n",
    "    train = pd.concat([train, dummies])\n",
    "    del dummies\n",
    "\n",
    "    month_last_day = train.groupby(\"date_block_num\").date.max().rename(\"month_last_day\")\n",
    "    month_last_day[~month_last_day.dt.is_month_end] = (\n",
    "        month_last_day[~month_last_day.dt.is_month_end] + MonthEnd()\n",
    "    )\n",
    "    month_first_day = train.groupby(\"date_block_num\").date.min().rename(\"month_first_day\")\n",
    "    month_first_day[~month_first_day.dt.is_month_start] = (\n",
    "        month_first_day[~month_first_day.dt.is_month_start] - MonthBegin()\n",
    "    )\n",
    "    month_length = (month_last_day - month_first_day + Day()).rename(\"month_length\")\n",
    "    first_shop_date = train.groupby(\"shop_id\").date.min().rename(\"first_shop_date\")\n",
    "    first_item_date = train.groupby(\"item_id\").date.min().rename(\"first_item_date\")\n",
    "    first_shop_item_date = (\n",
    "        train.groupby([\"shop_id\", \"item_id\"]).date.min().rename(\"first_shop_item_date\")\n",
    "    )\n",
    "    first_item_name_group_date = (\n",
    "        train.groupby(\"item_name_group\").date.min().rename(\"first_name_group_date\")\n",
    "    )\n",
    "\n",
    "    m = m.merge(month_first_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_last_day, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(month_length, left_on=\"date_block_num\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_shop_date, left_on=\"shop_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(first_item_date, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    m = m.merge(\n",
    "        first_shop_item_date, left_on=[\"shop_id\", \"item_id\"], right_index=True, how=\"left\"\n",
    "    )\n",
    "    m = m.merge(\n",
    "        first_item_name_group_date, left_on=\"item_name_group\", right_index=True, how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Calculate how long the item was sold for in each month and use this to calculate average sales per day\n",
    "    m[\"shop_open_days\"] = m[\"month_last_day\"] - m[\"first_shop_date\"] + Day()\n",
    "    m[\"item_first_sale_days\"] = m[\"month_last_day\"] - m[\"first_item_date\"] + Day()\n",
    "    m[\"item_in_shop_days\"] = (\n",
    "        m[[\"shop_open_days\", \"item_first_sale_days\", \"month_length\"]].min(axis=1).dt.days\n",
    "    )\n",
    "    m = m.drop(columns=\"item_first_sale_days\")\n",
    "    m[\"item_cnt_day_avg\"] = m[\"item_cnt_month\"] / m[\"item_in_shop_days\"]\n",
    "    m[\"month_length\"] = m[\"month_length\"].dt.days\n",
    "\n",
    "    # Calculate the time differences from the beginning of the month so they can be used as features without lagging\n",
    "    m[\"shop_open_days\"] = m[\"month_first_day\"] - m[\"first_shop_date\"]\n",
    "    m[\"first_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_item_date\"]\n",
    "    m[\"first_shop_item_sale_days\"] = m[\"month_first_day\"] - m[\"first_shop_item_date\"]\n",
    "    m[\"first_name_group_sale_days\"] = m[\"month_first_day\"] - m[\"first_name_group_date\"]\n",
    "    m[\"shop_open_days\"] = m[\"shop_open_days\"].dt.days.fillna(0).clip(lower=0)\n",
    "    m[\"first_item_sale_days\"] = (\n",
    "        m[\"first_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "    m[\"first_shop_item_sale_days\"] = (\n",
    "        m[\"first_shop_item_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "    m[\"first_name_group_sale_days\"] = (\n",
    "        m[\"first_name_group_sale_days\"].dt.days.fillna(0).clip(lower=0).replace(0, 9999)\n",
    "    )\n",
    "\n",
    "    # Add days since last sale\n",
    "    def last_sale_days(matrix):\n",
    "        last_shop_item_dates = []\n",
    "        for dbn in range(1, 35):\n",
    "            lsid_temp = (\n",
    "                train.query(f\"date_block_num<{dbn}\")\n",
    "                .groupby([\"shop_id\", \"item_id\"])\n",
    "                .date.max()\n",
    "                .rename(\"last_shop_item_sale_date\")\n",
    "                .reset_index()\n",
    "            )\n",
    "            lsid_temp[\"date_block_num\"] = dbn\n",
    "            last_shop_item_dates.append(lsid_temp)\n",
    "\n",
    "        last_shop_item_dates = pd.concat(last_shop_item_dates)\n",
    "        matrix = matrix.merge(\n",
    "            last_shop_item_dates, on=[\"date_block_num\", \"shop_id\", \"item_id\"], how=\"left\"\n",
    "        )\n",
    "\n",
    "        def days_since_last_feat(m, feat_name, date_feat_name, missingval):\n",
    "            m[feat_name] = (m[\"month_first_day\"] - m[date_feat_name]).dt.days\n",
    "            m.loc[m[feat_name] > 2000, feat_name] = missingval\n",
    "            m.loc[m[feat_name].isna(), feat_name] = missingval\n",
    "            return m\n",
    "\n",
    "        matrix = days_since_last_feat(\n",
    "            matrix, \"last_shop_item_sale_days\", \"last_shop_item_sale_date\", 9999\n",
    "        )\n",
    "\n",
    "        matrix = matrix.drop(columns=[\"last_shop_item_sale_date\"])\n",
    "        return matrix\n",
    "\n",
    "    m = last_sale_days(m)\n",
    "    # Month id feature\n",
    "    m[\"month\"] = m[\"month_first_day\"].dt.month\n",
    "\n",
    "    m = m.drop(\n",
    "        columns=[\n",
    "            \"first_day\",\n",
    "            \"month_first_day\",\n",
    "            \"month_last_day\",\n",
    "            \"first_shop_date\",\n",
    "            \"first_item_date\",\n",
    "            \"first_name_group_date\",\n",
    "            \"item_in_shop_days\",\n",
    "            \"first_shop_item_date\",\n",
    "            \"month_length\",\n",
    "        ],\n",
    "        errors=\"ignore\",\n",
    "    )\n",
    "\n",
    "    m = item_shop_age_months(m)\n",
    "\n",
    "    if correct_item_cnt_day == True:\n",
    "        m[\"item_cnt_month_original\"] = m[\"item_cnt_month\"]\n",
    "        m[\"item_cnt_month\"] = m[\"item_cnt_day_avg\"] * m[\"month_length\"]\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_time_features(matrix, train, False)\n",
    "print(\"Time features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price features\n",
    "전달 팔린 item 들의 가격과 관련된 다른 아이템들의 가격들입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_price_features(matrix, train):\n",
    "    # Get mean prices per month from train dataframe\n",
    "    price_features = train.groupby([\"date_block_num\", \"item_id\"]).item_price.mean()\n",
    "    price_features = pd.DataFrame(price_features)\n",
    "    price_features = price_features.reset_index()\n",
    "    # Calculate normalized differenced from mean category price per month\n",
    "    price_features = price_features.merge(\n",
    "        items[[\"item_id\", \"item_category_id\"]], how=\"left\", on=\"item_id\"\n",
    "    )\n",
    "    price_features[\"norm_diff_cat_price\"] = price_features.groupby(\n",
    "        [\"date_block_num\", \"item_category_id\"]\n",
    "    )[\"item_price\"].transform(lambda x: (x - x.mean()) / x.mean())\n",
    "    # Retain only the necessary features\n",
    "    price_features = price_features[\n",
    "        [\n",
    "            \"date_block_num\",\n",
    "            \"item_id\",\n",
    "            \"item_price\",\n",
    "            \"norm_diff_cat_price\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    features = [\n",
    "        \"item_price\",\n",
    "        \"norm_diff_cat_price\",\n",
    "    ]\n",
    "    newnames = [\"last_\" + f for f in features]\n",
    "    aggs = {f: \"last\" for f in features}\n",
    "    renames = {f: \"last_\" + f for f in features}\n",
    "    features = []\n",
    "    for dbn in range(1, 35):\n",
    "        f_temp = (\n",
    "            price_features.query(f\"date_block_num<{dbn}\")\n",
    "            .groupby(\"item_id\")\n",
    "            .agg(aggs)\n",
    "            .rename(columns=renames)\n",
    "        )\n",
    "        f_temp[\"date_block_num\"] = dbn\n",
    "        features.append(f_temp)\n",
    "    features = pd.concat(features).reset_index()\n",
    "    matrix = matrix.merge(features, on=[\"date_block_num\", \"item_id\"], how=\"left\")\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_price_features(matrix, train)\n",
    "del(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item category features  \n",
    "- 주어진 아이템 카테고리에 두 개의 그룹을 더해줬습니다. \n",
    "- 상위 그룹(예: \"games\", \"music\")과 플랫폼(예:  \"PS4\", \"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.merge(items[['item_id', 'item_category_id']], on='item_id', how='left')\n",
    "\n",
    "platform_map = {\n",
    "    0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 8, 10: 1, 11: 2,\n",
    "    12: 3, 13: 4, 14: 5, 15: 6, 16: 7, 17: 8, 18: 1, 19: 2, 20: 3, 21: 4, 22: 5,\n",
    "    23: 6, 24: 7, 25: 8, 26: 9, 27: 10, 28: 0, 29: 0, 30: 0, 31: 0, 32: 8, 33: 11,\n",
    "    34: 11, 35: 3, 36: 0, 37: 12, 38: 12, 39: 12, 40: 13, 41: 13, 42: 14, 43: 15,\n",
    "    44: 15, 45: 15, 46: 14, 47: 14, 48: 14, 49: 14, 50: 14, 51: 14, 52: 14, 53: 14,\n",
    "    54: 8, 55: 16, 56: 16, 57: 17, 58: 18, 59: 13, 60: 16, 61: 8, 62: 8, 63: 8, 64: 8,\n",
    "    65: 8, 66: 8, 67: 8, 68: 8, 69: 8, 70: 8, 71: 8, 72: 8, 73: 0, 74: 10, 75: 0,\n",
    "    76: 0, 77: 0, 78: 0, 79: 8, 80: 8, 81: 8, 82: 8, 83: 8,\n",
    "}\n",
    "matrix['platform_id'] = matrix['item_category_id'].map(platform_map)\n",
    "\n",
    "supercat_map = {\n",
    "    0: 0, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 1, 12: 1,\n",
    "    13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 3, 19: 3, 20: 3, 21: 3, 22: 3, 23: 3,\n",
    "    24: 3, 25: 0, 26: 2, 27: 3, 28: 3, 29: 3, 30: 3, 31: 3, 32: 2, 33: 2, 34: 2,\n",
    "    35: 2, 36: 2, 37: 4, 38: 4, 39: 4, 40: 4, 41: 4, 42: 5, 43: 5, 44: 5, 45: 5,\n",
    "    46: 5, 47: 5, 48: 5, 49: 5, 50: 5, 51: 5, 52: 5, 53: 5, 54: 5, 55: 6, 56: 6,\n",
    "    57: 6, 58: 6, 59: 6, 60: 6, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0,\n",
    "    68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 7, 74: 7, 75: 7, 76: 7, 77: 7, 78: 7,\n",
    "    79: 2, 80: 2, 81: 0, 82: 0, 83: 0\n",
    "}\n",
    "matrix['supercategory_id'] = matrix['item_category_id'].map(supercat_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shop city\n",
    "(from https://www.kaggle.com/dlarionov/feature-engineering-xgboost)\n",
    "\n",
    "기존 문서가 참조한 다른 문서의 특성 값입니다.\n",
    "\n",
    "항목의 설명을 보면 각각의 shop_name 이 각자의 도시 이름으로 시작하고 category 가 종류와 하위종류를 이름에 포함 중이라고 하는군요.\n",
    "하단의 코드는 이 특성을 분리하고 저장하는 코드입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_city_codes(matrix, shops):\n",
    "    shops.loc[\n",
    "        shops.shop_name == 'Сергиев Посад ТЦ \"7Я\"', \"shop_name\"\n",
    "    ] = 'СергиевПосад ТЦ \"7Я\"'\n",
    "    shops[\"city\"] = shops[\"shop_name\"].str.split(\" \").map(lambda x: x[0])\n",
    "    shops.loc[shops.city == \"!Якутск\", \"city\"] = \"Якутск\"\n",
    "    shops[\"city_code\"] = shops[\"city\"].factorize()[0]\n",
    "    shop_labels = shops[[\"shop_id\", \"city_code\"]]\n",
    "    matrix = matrix.merge(shop_labels, on='shop_id', how='left')\n",
    "    return matrix\n",
    "\n",
    "matrix = add_city_codes(matrix, shops)\n",
    "del(shops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shop and item 카테고리 군집화(category clustering)\n",
    "\n",
    "- shop 과 item 카테고리들은 sales 상태에 따라 군집화되어 있습니다.\n",
    "- 하단의 function 은 shop 과 item 카테고리들에 대해  주성분분석 분해 및 클러스터링(군집화)을 실시하고 그 결과를 그립니다. \n",
    "\n",
    "- 각 PCA 차원에 의해 설명되는 아이템 간의 설명 분산 비율이 표시되고 개별 아이템들이 PCA 차원의 점수에 따라 표시되고 군집그룹에 따라 색깔이 배정됩니다.\n",
    "\n",
    "- 파라미터의 다양한 값에 대한 'The silhouette score' (군집화 수준 점수) 역시 표시됩니다. 이런 그림들은 군집화 정도(군집화 그룹 수)를 결정하는데 사용됩니다. \n",
    "\n",
    "- shop 과 item 카테고리들에 대해 발생하는 차이의 80% 이상이 단일차원에서 발생합니다. 이는 차이가 주로 비율보다는 크기에 의해 발생한다는 사실을 나타냅니다. 항목 구성요소점수 그림은 클러스터링이 대부분의 항목을 포함하는 큰 군집과 이상값 항목을 몇 개 포함하는 작은 몇 개의 군집이 존재함을 나타냅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "def cluster_feature(matrix, target_feature, clust_feature, level_feature, n_components=4, n_clusters=5, aggfunc=\"mean\", exclude=None):\n",
    "    start_month = 20\n",
    "    end_month = 32\n",
    "    pt = matrix.query(f\"date_block_num>{start_month} & date_block_num<={end_month}\")\n",
    "    if exclude is not None:\n",
    "        pt = matrix[~matrix[clust_feature].isin(exclude)]\n",
    "    pt = pt.pivot_table(values=target_feature, columns=clust_feature, index=level_feature, fill_value=0, aggfunc=aggfunc)\n",
    "    pt = pt.transpose()\n",
    "    pca = PCA(n_components=10)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    # Plot PCA explained variance\n",
    "    sns.set_theme()\n",
    "    features = list(range(pca.n_components_))\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "    ax = fig.add_subplot(121)\n",
    "#     ax.bar(features, pca.explained_variance_ratio_, color=\"black\")\n",
    "    sns.barplot(x=features, y=pca.explained_variance_ratio_, ax=ax)\n",
    "    plt.title(\"Variance by PCA components\")\n",
    "    plt.xlabel(\"component\")\n",
    "    plt.ylabel(\"explained variance\")\n",
    "    plt.xticks(features)\n",
    "\n",
    "    scorelist = []\n",
    "    nrange = range(2, 10)\n",
    "    for n in nrange:\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n)\n",
    "        labels = clusterer.fit_predict(components)\n",
    "        silscore = silhouette_score(pt, labels)\n",
    "        scorelist.append(silscore)\n",
    "    ax = fig.add_subplot(122)\n",
    "    sns.lineplot(x=nrange, y=scorelist, ax=ax)\n",
    "    plt.title(\"Clustering quality by number of clusters\")\n",
    "    plt.xlabel(\"n clusters\")\n",
    "    plt.ylabel(\"silhouette score\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    components = pca.fit_transform(pt)\n",
    "    components = pd.DataFrame(components)\n",
    "    clusterer = AgglomerativeClustering(n_clusters=n_clusters, linkage=\"average\")\n",
    "    labels = clusterer.fit_predict(components)\n",
    "    x = components[0]\n",
    "    y = components[1]\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.scatterplot(x=x, y=y, hue=labels, palette=sns.color_palette(\"hls\", n_clusters), ax=ax)\n",
    "    plt.title(\"Items by cluster\")\n",
    "    plt.xlabel(\"component 1 score\")\n",
    "    plt.ylabel(\"component 2 score\")\n",
    "    for i, txt in enumerate(pt.index.to_list()):\n",
    "        ax.annotate(str(txt), (x[i], y[i]))\n",
    "    groups = {}\n",
    "    for i, s in enumerate(pt.index):\n",
    "        groups[s] = labels[i]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- item 카테고리들은 해당 연도의 매월 평균 판매량에 따라 군집합니다. \n",
    "- 주성분그림 그려보면 3 개의 카테고리가 이와 관련하여 이상값을 가지고 있음을 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_group_dict = cluster_feature(matrix, 'item_cnt_month', 'item_category_id', 'date_block_num', n_components=2, n_clusters=4, aggfunc=\"mean\", exclude =[])\n",
    "matrix['category_cluster'] = matrix['item_category_id'].map(category_group_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shop 은 각 item category의 총합 판매량으로 군집합니다.(=나뉘어져 그룹화됩니다.) 주성분그림를 그려보면 shop은 그들의 판매량에 의해 주로 차이를 가집니다. \n",
    "- Shop 31은 판매량으로 인해 이상치로 판정됩니다. Shop 12와 55는 서로 다른 (온라인 판매 전용) item을 팔고 있기 때문에 직교차원에서 이상치로 판정됩니다. (=서로 독립적이고 관련이 없습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_group_dict = cluster_feature(matrix, 'item_cnt_month', 'shop_id', 'item_category_id', n_components=4, n_clusters=4, aggfunc=\"mean\", exclude=[36])\n",
    "shop_group_dict[36] = shop_group_dict[37]  # Shop36 은 따로 더할 것인데 왜냐하면 한 달치 데이터만 가지고 있기 때문입니다.\n",
    "matrix['shop_cluster'] = matrix['shop_id'].map(shop_group_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)  # Use this function periodically to downcast dtypes to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단일 item 수에 의한 features\n",
    "\n",
    "- 이 특성들은 고유한 item의 숫자를 셉니다. (그룹화 특성이나 현재 item에 대한 현재 달의 특성들과 같이) 중요한 특성입니다. 예를 들어 같은 카테고리에 대해 새 item의 숫자가 얼마만큼 있는지에 대해서요. \n",
    "- (These features count the number of unique items sharing the same value of a grouping feature or set of features as the current item in the current month)\n",
    "\n",
    "\n",
    "- 이는 일종의 데이터 유출 기능으로 여겨질 수 있습니다, 각 달의 item set이 예측한 달에 팔렸을지 안 팔렸을지 미리 알 수 없는 것처럼 말입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniques(matrix, groupers, name, limitation=None):\n",
    "    if limitation is not None:\n",
    "        s = (\n",
    "            matrix.query(limitation)\n",
    "            .groupby(groupers)\n",
    "            .item_id.nunique()\n",
    "            .rename(name)\n",
    "            .reset_index()\n",
    "        )\n",
    "    else:\n",
    "        s = matrix.groupby(groupers).item_id.nunique().rename(name).reset_index()\n",
    "    matrix = matrix.merge(s, on=groupers, how=\"left\")\n",
    "    matrix[name] = matrix[name].fillna(0)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"unique_items_month\")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_name_group\"], \"name_group_unique_month\")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n",
    "    \"name_group_cat_unique_month\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_name_group\"],\n",
    "    \"name_group_new_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"item_name_group\"],\n",
    "    \"name_group_new_cat_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "\n",
    "matrix = uniques(\n",
    "    matrix, [\"date_block_num\", \"artist_name_or_first_word\"], \"first_word_unique_month\"\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_cat_unique_month\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_new_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\", \"artist_name_or_first_word\"],\n",
    "    \"first_word_new_cat_unique_month\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "\n",
    "matrix = uniques(matrix, [\"date_block_num\", \"item_category_id\"], \"unique_items_cat\")\n",
    "matrix = uniques(\n",
    "    matrix,\n",
    "    [\"date_block_num\", \"item_category_id\"],\n",
    "    \"new_items_cat\",\n",
    "    limitation=\"new_item==True\",\n",
    ")\n",
    "matrix = uniques(matrix, [\"date_block_num\"], \"new_items_month\", limitation=\"new_item==True\")\n",
    "\n",
    "matrix[\"cat_items_proportion\"] = matrix[\"unique_items_cat\"] / matrix[\"unique_items_month\"]\n",
    "matrix[\"name_group_new_proportion_month\"] = (\n",
    "    matrix[\"name_group_new_unique_month\"] / matrix[\"name_group_unique_month\"]\n",
    ")\n",
    "\n",
    "matrix = matrix.drop(columns=[\"unique_items_month\", \"name_group_unique_month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 집계량 퍼센트화   \n",
    "- 이는 Pandas 패키지의 pct_change 메서드를 사용해서 특정 그룹의 특정 시간대에 대한 평균 판매수의 비율적 변화를 계산합니다. 예를 들어 '지난 2개월 간 특정 item의 평균판매수의 증가/변화 에 따른 내용' 같이요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pct_change(\n",
    "    matrix,\n",
    "    group_feats,\n",
    "    target=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    periods=1,\n",
    "    lag=1,\n",
    "    clip_value=None,\n",
    "):\n",
    "    periods = list_if_not(periods, int)\n",
    "    group_feats = list_if_not(group_feats)\n",
    "    group_feats_full = [\"date_block_num\"] + group_feats\n",
    "    dat = matrix.pivot_table(\n",
    "        index=group_feats + [\"date_block_num\"],\n",
    "        values=target,\n",
    "        aggfunc=aggfunc,\n",
    "        fill_value=0,\n",
    "        dropna=False,\n",
    "    ).astype(\"float32\")\n",
    "    for g in group_feats:\n",
    "        firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "        dat = dat.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "        dat.loc[dat.index.get_level_values(\"date_block_num\") < dat[\"firsts\"], target] = float(\n",
    "            \"nan\"\n",
    "        )\n",
    "        del dat[\"firsts\"]\n",
    "    for period in periods:\n",
    "        feat_name = \"_\".join(\n",
    "            group_feats + [target] + [aggfunc] + [\"delta\"] + [str(period)] + [f\"lag_{lag}\"]\n",
    "        )\n",
    "        print(f\"Adding feature {feat_name}\")\n",
    "        dat = (\n",
    "            dat.groupby(group_feats)[target]\n",
    "            .transform(lambda x: x.pct_change(periods=period, fill_method=\"pad\"))\n",
    "            .rename(feat_name)\n",
    "        )\n",
    "        if clip_value is not None:\n",
    "            dat = dat.clip(lower=-clip_value, upper=clip_value)\n",
    "    dat = dat.reset_index()\n",
    "    dat[\"date_block_num\"] += lag\n",
    "    matrix = matrix.merge(dat, on=[\"date_block_num\"] + group_feats, how=\"left\")\n",
    "    matrix[feat_name] = reduce_mem_usage(matrix[feat_name])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_pct_change(matrix, [\"item_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", clip_value=3)\n",
    "matrix = add_pct_change(matrix, [\"item_name_group\"], \"item_cnt_month\", clip_value=3)\n",
    "# Delta 1 feature lagged by 12 months, intended to capture seasonal trends\n",
    "matrix = add_pct_change(matrix, [\"item_category_id\"], \"item_cnt_month\", lag=12, clip_value=3,)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +)Window aggregation\n",
    "\n",
    "- 전체 데이터의 볼륨이 클 경우 이를 윈도우 단위로 잘라서 처리할 수 있는 방법이 있는데, 예를 들어 10분 단위로 데이터를 처리해서, 10분 단위로 최소/최대 값을 구하거나 또는 10분 단위로 어떤 값의 평균값을 대표값으로 사용하는 것들이 이에 해당한다. \n",
    "\n",
    "- 일반적으로 입력값은 (entity, timestamp, value) 형태가 되며, 전처리된 출력 값은 다음과 같이. (entity, time_index, aggregated_value_over_time_window) 엔터티(피쳐)에 대해서 윈도우별로 처리된 값을 저장하는 형태가 된다.  보통 이런 window aggregation 방식은 리얼 타임 스트리밍 데이터에서 시간 윈도우 단위로 데이터를 처리하는 경우에 많이 사용이 되며, Apache Beam과 같은 스트리밍 프레임워크를 이용하여 구현한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowed aggregates\n",
    "\n",
    "+) 일정 시간간격동안의 간격 = window\n",
    "\n",
    "일종의 time filter 라고 보는 게 편하겠습니다. ex)window_size = n # n일간의 데이터를 갖고 다음 단일 타겟을 예측\n",
    "\n",
    "noise(오차)를 줄이기 위해 특정 윈도우들을 집계한 특성입니다. \n",
    "\n",
    "접근가능한 window는 확장(=앞선 모든 시점들), 롤링(=동일한 가중치가 부여된 고정된 각 시점에 대한 가중치들) 및 지수가중평균이 있습니다.\n",
    "\n",
    "A note about feature names: these are set automatically according to the pattern < grouping features > - < aggregated features > - < monthly aggregation function > - < window type > , where < window type > is either \"rolling - < window aggregation function > - win - < window length in months >\" for square rolling windows, \"expanding - < window aggregation function >\" for expanding windows, and \"ewm_hl - < decay rate in terms of half-life > for exponential weighted means, all connected by underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id = 16\n",
    "item_id = 482\n",
    "im = matrix.query(f\"shop_id=={shop_id} & item_id=={item_id}\")[['date_block_num', 'item_cnt_month']]\n",
    "im['moving average'] = im['item_cnt_month'].ewm(halflife=1).mean()\n",
    "im['expanding mean'] = im['item_cnt_month'].expanding().mean()\n",
    "im['rolling 12 month mean'] = im['item_cnt_month'].rolling(window=12, min_periods=1).mean()\n",
    "im = im.set_index('date_block_num')\n",
    "ax = im.plot(figsize=(12,5), marker='.', title='Time series averaging methods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_stats(\n",
    "    matrix,\n",
    "    features,\n",
    "    window=12,\n",
    "    kind=\"rolling\",\n",
    "    argfeat=\"item_cnt_month\",\n",
    "    aggfunc=\"mean\",\n",
    "    rolling_aggfunc=\"mean\",\n",
    "    dtype=\"float16\",\n",
    "    reshape_source=True,\n",
    "    lag_offset=0,\n",
    "):\n",
    "    def rolling_stat(\n",
    "        matrix,\n",
    "        source,\n",
    "        feats,\n",
    "        feat_name,\n",
    "        window=12,\n",
    "        argfeat=\"item_cnt_month\",\n",
    "        aggfunc=\"mean\",\n",
    "        dtype=dtype,\n",
    "        lag_offset=0,\n",
    "    ):\n",
    "        # Calculate a statistic on a windowed section of a source table,  grouping on specific features\n",
    "        store = []\n",
    "        for i in range(2 + lag_offset, 35 + lag_offset):\n",
    "            if len(feats) > 0:\n",
    "                mes = (\n",
    "                    source[source.date_block_num.isin(range(max([i - window, 0]), i))]\n",
    "                    .groupby(feats)[argfeat]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                    .rename(feat_name)\n",
    "                    .reset_index()\n",
    "                )\n",
    "            else:\n",
    "                mes = {}\n",
    "                mes[feat_name] = (\n",
    "                    source.loc[\n",
    "                        source.date_block_num.isin(range(max([i - window, 0]), i)), argfeat\n",
    "                    ]\n",
    "                    .agg(aggfunc)\n",
    "                    .astype(dtype)\n",
    "                )\n",
    "                mes = pd.DataFrame(data=mes, index=[i])\n",
    "            mes[\"date_block_num\"] = i - lag_offset\n",
    "            store.append(mes)\n",
    "        store = pd.concat(store)\n",
    "        matrix = matrix.merge(store, on=feats + [\"date_block_num\"], how=\"left\")\n",
    "        return matrix\n",
    "\n",
    "    \"\"\" An issue when using windowed functions is that missing values from months when items recorded no sales are skipped rather than being correctly\n",
    "    treated as zeroes. Creating a pivot_table fills in the zeros.\"\"\"\n",
    "    if (reshape_source == True) or (kind == \"ewm\"):\n",
    "        source = matrix.pivot_table(\n",
    "            index=features + [\"date_block_num\"],\n",
    "            values=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            fill_value=0,\n",
    "            dropna=False,\n",
    "        ).astype(dtype)\n",
    "        for g in features:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            source = source.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            # Set values before the items first appearance to nan so they are ignored rather than being treated as zero sales.\n",
    "            source.loc[\n",
    "                source.index.get_level_values(\"date_block_num\") < source[\"firsts\"], argfeat\n",
    "            ] = float(\"nan\")\n",
    "            del source[\"firsts\"]\n",
    "        source = source.reset_index()\n",
    "    else:\n",
    "        source = matrix\n",
    "\n",
    "    if kind == \"rolling\":\n",
    "        feat_name = (\n",
    "            f\"{'_'.join(features)}_{argfeat}_{aggfunc}_rolling_{rolling_aggfunc}_win_{window}\"\n",
    "        )\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            matrix,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=window,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=rolling_aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"expanding\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_expanding_{rolling_aggfunc}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        return rolling_stat(\n",
    "            matrix,\n",
    "            source,\n",
    "            features,\n",
    "            feat_name,\n",
    "            window=100,\n",
    "            argfeat=argfeat,\n",
    "            aggfunc=aggfunc,\n",
    "            dtype=dtype,\n",
    "            lag_offset=lag_offset,\n",
    "        )\n",
    "    elif kind == \"ewm\":\n",
    "        feat_name = f\"{'_'.join(features)}_{argfeat}_{aggfunc}_ewm_hl_{window}\"\n",
    "        print(f'Creating feature \"{feat_name}\"')\n",
    "        source[feat_name] = (\n",
    "            source.groupby(features)[argfeat]\n",
    "            .ewm(halflife=window, min_periods=1)\n",
    "            .agg(rolling_aggfunc)\n",
    "            .to_numpy(dtype=dtype)\n",
    "        )\n",
    "        del source[argfeat]\n",
    "        #         source = source.reset_index()\n",
    "        source[\"date_block_num\"] += 1 - lag_offset\n",
    "        return matrix.merge(source, on=[\"date_block_num\"] + features, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "롤링 특성 만들기. \n",
    "\n",
    "그룹 특성과 윈도우 타입을 조합하여 만들어진 많은 특성들을 scikit-learn RFECV function을 사용하여 가지치기(pruning) 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"artist_name_or_first_word\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"category_cluster\"], window=12)\n",
    "matrix = add_rolling_stats(\\\n",
    "                           \n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_category_id\", \"new_item\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_category_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"shop_id\", \"item_id\"], window=12)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\", \"item_category_id\", \"new_item\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"item_name_group\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_id\", \"supercategory_id\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"shop_cluster\", \"item_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_cluster\", \"item_category_id\", \"item_age\"],\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"shop_cluster\", \"item_name_group\", \"new_item\"], window=12, reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"category_cluster\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"item_id\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"artist_name_or_first_word\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"artist_name_or_first_word\", \"item_category_id\", \"item_age\"],\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"artist_name_or_first_word\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"item_age\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"item_category_id\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_category_id\", \"new_item\"], kind=\"expanding\", reshape_source=False\n",
    ")\n",
    "\n",
    "matrix = add_rolling_stats(\n",
    "    matrix, [\"item_name_group\", \"item_age\"], window=12, reshape_source=False\n",
    ")\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], kind=\"ewm\", window=1)\n",
    "matrix = add_rolling_stats(matrix, [\"item_name_group\"], window=12)\n",
    "\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], window=12)\n",
    "matrix = add_rolling_stats(matrix, [\"platform_id\"], kind=\"ewm\", window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이하의 코드들은 window의 평균 판매 특성들을 일자별로 정확도(resolution)를 나타낸 코드입니다.\n",
    "\n",
    "엄밀히 말하면 resolution(분해능)은 정확도와 다른 개념이지만, 편리를 위해 정확도라고 번역하겠습니다.\n",
    "\n",
    "+) Entity Resolution is a technique to identify data records in a single data source or across multiple data sources that refer to the same real-world entity and to link the records together.\n",
    "\n",
    "https://m.blog.naver.com/instron_kor/220325603719  <- 설명을 참조하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summed sales & accurate windowed mean sales per day features\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"rolling\",\n",
    "    window=12,\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_id\"],\n",
    "    aggfunc=\"sum\",\n",
    "    rolling_aggfunc=\"sum\",\n",
    "    kind=\"expanding\",\n",
    "    reshape_source=False,\n",
    ")\n",
    "matrix[\"1year\"] = 365\n",
    "matrix[\"item_id_day_mean_expanding\"] = matrix[\n",
    "    \"item_id_item_cnt_month_sum_expanding_sum\"\n",
    "] / matrix[[\"first_item_sale_days\"]].min(axis=1)\n",
    "matrix[\"shop_id_item_id_day_mean_win_12\"] = matrix[\n",
    "    \"shop_id_item_id_item_cnt_month_sum_rolling_sum_win_12\"\n",
    "] / matrix[[\"first_item_sale_days\", \"shop_open_days\", \"1year\"]].min(axis=1)\n",
    "matrix.loc[matrix.new_item == True, \"item_id_day_mean_expanding\",] = float(\"nan\")\n",
    "matrix = matrix.drop(columns=[\"1year\", \"item_id_item_cnt_month_sum_expanding_sum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"shop_id\", \"item_name_group\"],\n",
    "    window=12,\n",
    "    argfeat=\"item_revenue_month\",\n",
    "    dtype=\"float32\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "작년에 대한 윈도우 평균 unique item feature 과 카테고리 내 새 item의 비율과 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_category_id\"],\n",
    "    argfeat=\"new_items_cat\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")\n",
    "matrix = add_rolling_stats(\n",
    "    matrix,\n",
    "    [\"item_name_group\"],\n",
    "    argfeat=\"name_group_new_unique_month\",\n",
    "    window=12,\n",
    "    reshape_source=True,\n",
    "    lag_offset=1,\n",
    ")\n",
    "\n",
    "matrix[\"new_items_cat_1_12_ratio\"] = (\n",
    "    matrix[\"new_items_cat\"]\n",
    "    / matrix[\"item_category_id_new_items_cat_mean_rolling_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged features and mean encodings(시차 특성들과 mean encoding)\n",
    "이전 달의 동일한 shop-item 조합에 대한 값들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_lag_feature(matrix, lag_feature, lags):\n",
    "    for lag in lags:\n",
    "        newname = lag_feature + f\"_lag_{lag}\"\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n",
    "        targetseries[\"date_block_num\"] += lag\n",
    "        targetseries = targetseries.rename(columns={lag_feature: newname})\n",
    "        matrix = matrix.merge(\n",
    "            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n",
    "        )\n",
    "        matrix.loc[\n",
    "            (matrix.item_age >= lag) & (matrix.shop_age >= lag) & (matrix[newname].isna()),\n",
    "            newname,\n",
    "        ] = 0\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = simple_lag_feature(matrix, 'item_cnt_month', lags=[1,2,3])\n",
    "matrix = simple_lag_feature(matrix, 'item_cnt_day_avg', lags=[1, 2, 3])\n",
    "matrix = simple_lag_feature(matrix, 'item_revenue_month', lags=[1])\n",
    "gc.collect()\n",
    "print(\"Lag features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean encoding(역자 첨부)\n",
    "\n",
    "개요\n",
    "\n",
    "1) 많은 distincts 변수들을 다룰 때, 많이 사용되는 기법으로 특히 tree-based 모델에서 유용하다.\n",
    "\n",
    "2) 예측 : 각 category별로 평균 target 값을 세서 class의 label로 사용\n",
    "\n",
    "3) 분류 : data point가 class 중 하나에 속할 가능성을 세서 class의 label로 사용\n",
    "\n",
    "4) label encoding과 매우 유사하지만 target 값과 상관관계가 있어서 학습시키는데 좀 더 효율적이다.\n",
    "\n",
    " $$label_c=p_c$$\n",
    "- p_c는 category_c의 target value의 평균값을 의미\n",
    "\n",
    "**주의점**\n",
    "\n",
    "- 학습데이터에 예측 값이 포함되기 때문에 train data에만 적용\n",
    "\n",
    "- test data는 절대 사용하면 안된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Encoding \n",
    "\n",
    "각 카테고리 특성 수준에 따른 타겟 특성에 대한 평균 또는 총합 값과 카테고리의 특성들을 조합해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_apply_ME(\n",
    "    matrix, grouping_fields, lags=[1], target=\"item_cnt_day_avg\", aggfunc=\"mean\"\n",
    "):\n",
    "    grouping_fields = list_if_not(grouping_fields)\n",
    "    for lag in lags:\n",
    "        newname = \"_\".join(grouping_fields + [target] + [aggfunc] + [f\"lag_{lag}\"])\n",
    "        print(f\"Adding feature {newname}\")\n",
    "        me_series = (\n",
    "            matrix.groupby([\"date_block_num\"] + grouping_fields)[target]\n",
    "            .agg(aggfunc)\n",
    "            .rename(newname)\n",
    "            .reset_index()\n",
    "        )\n",
    "        me_series[\"date_block_num\"] += lag\n",
    "        matrix = matrix.merge(me_series, on=[\"date_block_num\"] + grouping_fields, how=\"left\")\n",
    "        del me_series\n",
    "        matrix[newname] = matrix[newname].fillna(0)\n",
    "        for g in grouping_fields:\n",
    "            firsts = matrix.groupby(g).date_block_num.min().rename(\"firsts\")\n",
    "            matrix = matrix.merge(firsts, left_on=g, right_index=True, how=\"left\")\n",
    "            matrix.loc[\n",
    "                matrix[\"date_block_num\"] < (matrix[\"firsts\"] + (lag)), newname\n",
    "            ] = float(\"nan\")\n",
    "            del matrix[\"firsts\"]\n",
    "        matrix[newname] = reduce_mem_usage(matrix[newname])\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"], target=\"item_cnt_month\", aggfunc=\"sum\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_name_group\"])\n",
    "matrix = create_apply_ME(matrix, [\"platform_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"supercategory_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"item_category_id\", \"new_item\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_id\", \"item_category_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"], target=\"item_cnt_month\")\n",
    "matrix = create_apply_ME(matrix, [\"shop_cluster\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_id\"])\n",
    "matrix = create_apply_ME(matrix, [\"city_code\", \"item_name_group\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lag 1 sales와 rolling 12개월 평균 사이의 비율은 이전 평균의 감소를 포착하기 위한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix[\"item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_id_item_cnt_month_mean_lag_1\"]\n",
    "    / matrix[\"item_id_item_cnt_month_mean_rolling_mean_win_12\"]\n",
    ")\n",
    "matrix[\"shop_id_item_id_item_cnt_1_12_ratio\"] = (\n",
    "    matrix[\"item_cnt_day_avg_lag_1\"] / matrix[\"shop_id_item_id_day_mean_win_12\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, oldcols = shrink_mem_new_cols(matrix, oldcols)\n",
    "matrix.to_pickle(\"matrixcheckpoint.pkl\")\n",
    "print(\"Saved matrixcheckpoint\")\n",
    "gc.collect()\n",
    "print(\"Mean encoding features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇몇 column 들은 다른 feature을 만들기 위해 제작되었으나 현재는 폐기되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surplus_columns = [\n",
    "    \"item_revenue_month\",\n",
    "    \"item_cnt_day_avg\",\n",
    "    \"item_name_group\",\n",
    "    \"artist_name_or_first_word\",\n",
    "    \"item_age\",\n",
    "    \"shop_open_days\",\n",
    "    \"shop_age\",\n",
    "    \"platform_id\",\n",
    "    \"supercategory_id\",\n",
    "    \"city_code\",\n",
    "    \"category_cluster\",\n",
    "    \"shop_cluster\",\n",
    "    \"new_items_cat\",\n",
    "    \"shop_id_item_id_day_mean_win_12\",\n",
    "    \"item_id_item_cnt_month_mean_rolling_mean_win_12\",\n",
    "]\n",
    "matrix = matrix.drop(columns=surplus_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive words in item_name\n",
    "\n",
    "One-hot feautre 은 item_name 단어들로 item sales를 예측하기 위해 만들어졌습니다. \n",
    "\n",
    "항목 이름에서 찾은 1000 개의 단어에서 *k* 단어 feature을 선택하기 위해 item의 임계값 수의 이름에 없거나 test 나 validation month에 없는 item 이름들은 버려집니다. \n",
    "\n",
    "남은 단어들은 scikit-learn SelectKBest function 에 의해 그들의 타겟과의 상관관계에 따라 선택됩니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"sklearn\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "def name_token_feats(matrix, items, k=50, item_n_threshold=5, target_month_start=33):\n",
    "    def name_correction(st):\n",
    "        st = re.sub(r\"[^\\w\\s]\", \"\", st)\n",
    "        st = re.sub(r\"\\s{2,}\", \" \", st)\n",
    "        st = st.lower().strip()\n",
    "        return st\n",
    "\n",
    "    items[\"item_name_clean\"] = items[\"item_name\"].apply(name_correction)\n",
    "\n",
    "    def create_item_id_bow_matrix(items):\n",
    "        all_stopwords = stopwords.words(\"russian\")\n",
    "        all_stopwords = all_stopwords + stopwords.words(\"english\")\n",
    "\n",
    "        vectorizer = CountVectorizer(stop_words=all_stopwords)\n",
    "        X = vectorizer.fit_transform(items.loc[:, \"item_name_clean\"])\n",
    "        X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "        print(f\"{len(vectorizer.vocabulary_)} words found in all items\")\n",
    "        featuremap = {\n",
    "            col: \"word_\" + token\n",
    "            for col, token in zip(\n",
    "                range(len(vectorizer.vocabulary_)), vectorizer.get_feature_names()\n",
    "            )\n",
    "        }\n",
    "        X = X.rename(columns=featuremap)\n",
    "        return X\n",
    "\n",
    "    items_bow = create_item_id_bow_matrix(items)\n",
    "    items_bow = items_bow.clip(0, 1)  # Made the word counts binary\n",
    "    common_word_mask = items_bow.sum(axis=0) > item_n_threshold\n",
    "    target_items = matrix.query(\n",
    "        f\"date_block_num>={target_month_start} & new_item==True\"\n",
    "    ).item_id.unique()\n",
    "    target_item_mask = items_bow.loc[target_items, :].sum(axis=0) > 1\n",
    "    items_bow = items_bow.loc[:, common_word_mask & target_item_mask]\n",
    "    print(f\"{items_bow.shape[1]} words of interest\")\n",
    "    mxbow = matrix[[\"date_block_num\", \"item_id\", \"item_cnt_month\"]].query(\"date_block_num<34\")\n",
    "    mxbow = mxbow.merge(items_bow, left_on=\"item_id\", right_index=True, how=\"left\")\n",
    "    X = mxbow.drop(columns=[\"date_block_num\", \"item_id\", \"item_cnt_month\"])\n",
    "    y = mxbow[\"item_cnt_month\"].clip(0, 20)\n",
    "    selektor = SelectKBest(f_regression, k=k)\n",
    "    selektor.fit(X, y)\n",
    "    tokencols = X.columns[selektor.get_support()]\n",
    "    print(f\"{k} word features selected\")\n",
    "    return items_bow[tokencols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(path+\"items.csv\")\n",
    "word_frame = name_token_feats(matrix, items, k=50, item_n_threshold=5)\n",
    "matrix = matrix.merge(word_frame, left_on='item_id', right_index=True, how='left')\n",
    "# LightGBM didn't seem to work with sparse features in this case, so we'll convert them to int\n",
    "# sparse data는 차원/전체 공간에 비해 데이터가 있는 공간이 매우 협소한 데이터를 의미합니다. 희소행렬을 생각해보세요.\n",
    "sparsecols = [c for c in matrix.columns if pd.api.types.is_sparse(matrix[c].dtype)]\n",
    "matrix[sparsecols] = matrix[sparsecols].sparse.to_dense().astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 feature frame은 저장한 후 노트북 커널을 리셋하여 LightGBM 을 돌리기 위한 메모리를 확보합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "matrix.to_pickle(\"checkpoint_final_0.84.pkl\")\n",
    "print(\"All features generated, dataframe saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model fitting part\n",
    "이하는 아직 쓰지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train x, y split -> test data doesn't need it.\n",
    "\n",
    "train_x = train.drop(['item_cnt_day'],axis =1)\n",
    "train_y = train['item_cnt_day']\n",
    "test_x = test.copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
